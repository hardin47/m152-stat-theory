[
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Statistical Theory",
    "section": "",
    "text": "Clicker Q\nto go with Probability & Statistics by DeGroot and Schervish. Math 152 - Statistical Theory.\n\n\n\nThe Central Limit Theorem (CLT) says:1\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nWhich cab company was involved (see example 2.2 in the notes)?2\n\nVery likely the Blue Cab company\nSort of likely the Blue Cab company\nEqually likely Blue and Green Cab companies\nSort of likely the Green Cab company\nVery likely the Green Cab company\n\n\n\n\nConsider a continuous probability density function (pdf) given by \\(f( x | \\theta ).\\) Which of the following is FALSE:3\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\\(f( x | \\theta )\\) provides info for calculating probabilities of X.\n\\(P(X = x) = 0\\) if X is continuous.\n\\(f( x | \\theta ) = L(\\theta | x)\\) is the likelihood function\n\n\n\n\nTo find a marginal distribution of X from a joint distribution of X & Y you should (assume everything is continuous),4\n\ndifferentiate the joint distribution with respect to X.\ndifferentiate the joint distribution with respect to Y.\nintegrate the joint distribution with respect to X.\nintegrate the joint distribution with respect to Y.\nI have no idea what a marginal distribution is.\n\n\n\n\nA continuous pdf (of a random variable \\(X\\) with parameter \\(\\theta\\)) should5\n\nIntegrate to a constant (\\(dx\\))\nIntegrate to a constant (\\(d\\theta\\))\nIntegrate to 1 (\\(dx\\))\nIntegrate to 1 (\\(d\\theta\\))\nnot need to integrate to anything special.\n\n\n\n\nR / R Studio\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “R”?\n\n\n\n\nIn terms of the R for the homework…\n\nI was able to do the whole thing.\nI understood the code part, but I couldn’t get the Markdown file to compile.\nI didn’t understand the code at all.\nI couldn’t get R or R Studio installed.\nI haven’t tried to work on the homework yet.\n\n\n\n\nA beta distribution6\n\nhas support on [0,1]\nhas parameters \\(\\alpha\\) and \\(\\beta\\) which represent, respectively, the mean and variance\nis discrete\nhas equal mean and variance\nhas equal mean and standard deviation\n\n\n\n\nWhat types of distributions are the following?7\n\nprior = marginal & posterior = joint\nprior = joint & posterior = conditional\nprior = conditional & posterior = joint\nprior = marginal & posterior = conditional\nprior = joint & posterior = marginal\n\n\n\n\nWhich of these are incorrect conclusions?8\n\n\\(\\theta | \\underline{X} \\sim\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\propto \\theta^{4-1} (1-\\theta)^{12-1}\\)\n\\(\\xi(\\theta | \\underline{X}) = \\frac{1}{B(4,12)} \\theta^{4-1}(1-\\theta)^{12-1}\\)\n\n\n\n\nWhat is the integrating constant for the pdf, \\(h(w)\\)?9\n\n\\(\\frac{\\Gamma(w+k)}{\\Gamma(w)\\Gamma(k)}\\)\n1/[\\(w^k \\Gamma(k)\\)]\n1 / \\(\\sqrt{2\\pi k^2}\\)\n1/[\\(\\Gamma(k/2)\\)]\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n\n\\[h(w) \\propto w^{k/2-1}e^{-w/2} \\ \\ \\ \\ \\ \\ \\ \\ \\ w>0\\]\n\n\nSuppose the data come from an exponential distribution with a parameter whose prior is given by a gamma distribution. The posterior is known to be conjugate, so its distribution must be in what family?10\n\nexponential\ngamma\nnormal\nbeta\nPoisson\n\n\n\n\nA prior is improper if11\n\nit conveys no real information.\nit isn’t conjugate.\nit doesn’t integrate to one.\nit swears a lot.\nit isn’t on your distribution sheet.\n\n\n\n\nGiven a prior: \\(\\theta \\sim N(\\mu_0, \\nu_0^2)\\)\nAnd a data likelihood: \\(X | \\theta \\sim N(\\theta, \\sigma^2)\\)\nYou collect n data values, what is your best guess of \\(\\theta?\\)12\n\n\\(\\overline{X}\\)\n\\(\\mu_0\\)\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0^2 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)\n\nmedian of \\(N(\\mu_1, \\nu_1^2 = \\frac{\\sigma^2 \\nu_0^2}{\\sigma^2 + n \\nu_0^2})\\)\n\n47\n\n\n\n\n\nThe Bayes estimator is sensitive to13\n\nthe posterior mean\nthe prior mean\nthe sample size\nthe data values\nsome of the above\n\n\n\n\nThe range (output) of the Bayesian MSE includes:14\n\ntheta\nthe data\n\n\n\n\nThe range (output) of the frequentist MSE includes:15\n\ntheta\nthe data\n\n\n\n\nTo find the maximum likelihood estimator, we take the derivative of the likelihood16\n\nwith respect to \\(X\\)\nwith respect to \\(\\underline{X}\\)\nwith respect to \\(\\theta\\)\nwith respect to \\(f\\)\nwith respect to \\(\\ln(f)\\)\n\n\n\n\nConsider an MLE, \\(\\hat{\\theta},\\) and the related log likelihood function \\(L = \\ln(f).\\) \\(\\delta(X)\\) is another estimate of \\(\\theta\\). Which statement is necessarily false:17\n\nL(\\(\\delta(X)\\)) < L(\\(\\theta\\))\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))\nL(\\(\\theta\\)) < L(\\(\\delta(X)\\))\nL(\\(\\delta(X)\\)) < L(\\(\\hat{\\theta}\\))\nL(\\(\\theta\\)) < L(\\(\\hat{\\theta}\\))\n\n\n\n\nThe MLE is popular because it18\n\nmaximizes \\(R^2\\)\nminimizes the sum of squared errors\nhas desirable sampling distribution properties\nmaximizes both the likelihood and the log likelihood\nalways exists\n\n\n\n\nMOM is popular because it:19\n\nhas desirable sampling properties\nis often straightforward to compute\nalways produces values inside the parameter space (e.g., in [0,1] for prob)\nalways exists\n\n\n\n\nThe Central Limit Theorem (CLT) says:20\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nA sampling distribution is21\n\nthe true distribution of the data\nthe estimated distribution of the data\nthe distribution of the population\nthe distribution of the statistic in repeated samples\nthe distribution of the statistic from your one sample of data\n\n\n\n\nThe distribution of a random variable can be uniquely determined by22\n\nthe cdf: F(x)\nthe pdf (pmf): f(x)\nthe moment generating function (mgf), if it exists: \\(\\Psi(t) = E[e^{tX}]\\)\nthe mean and variance of the distribution\nmore than one of the above (which ones??)\n\n\n\n\nA moment generating function23\n\ngives the probability of the RV at any value of X\ngives all theoretical moments of the distribution\ngives all sample moments of the data\ngives the cumulative probability of the RV at any value of X\n\n\n\n\nThe sampling distribution is important because24\n\nit describes the behavior (distribution) of the statistic\nit describes the behavior (distribution) of the data\nit gives us the ability to measure the likelihood of the statistic or more extreme under particular settings (i.e. null)\nit gives us the ability to make inferences about the population parameter\nmore than one of the above (which ones??)\n\n\n\n\nThe following result: \\(\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) allows us to isolate and conduct inference on what parameter?25\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nThe following result: \\(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\) allows us to isolate and conduct inference on what parameter?26\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nWhat would you expect the standard deviation of the t statistic to be?27\n\na little bit less than 1\n1\na little bit more than 1\nunable to tell because it depends on the sample size and the variability of the data\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. What is the sample size of each bootstrap sample?28\n\n50\n1000\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. How many bootstrap statistics will you have?29\n\n50\n1000\n\n\n\n\nThe bootstrap distribution of \\(\\hat{\\theta}\\) is centered around the30\n\npopulation parameter\nsample statistic\nbootstrap statistic\nbootstrap parameter\n\n\n\n\nThe bootstrap theory relies on31\n\nResampling with replacement from the original sample.\nResampling from the original sample, leaving one observation out each time (e.g., cross validation)\nEstimating the population using the sample.\nPermuting the data values within the sample.\n\n\n\n\nBias of a statistic refers to32\n\nThe difference between a statistic and the actual parameter\nWhether or not questions were worded fairly.\nThe difference between a sampling distribution mean and the actual parameter.\n\n\n\n\nThe mean of a sample is 22.5. The mean of 1000 bootstrapped samples is 22.491. The bias of the bootstrap mean is33\n\n-0.009\n-0.0045\n-0.09\n0.009\n0.09\n\n\n\n\nThe following result: \\(\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) allows us to isolate and conduct inference on what parameter?34\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nThe following result: \\(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\)\nallows us to isolate and conduct inference on what parameter?35\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nConsider an asymmetric confidence interval for \\(\\sigma\\) which is derived using:\n\\(P(c_1 \\leq \\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{\\sigma^2} \\leq c_2) = 0.95\\)\nThe resulting 95% interval with the shortest width has:36\n\n\\(c_1\\) and \\(c_2\\) as the .025 & .975 quantiles\n\\(c_1\\) set to zero\n\\(c_2\\) set to infinity\n\\(c_1\\) and \\(c_2\\) as different quantiles than (a) but that contain .95 probability.\nFind \\(c_1\\) and let \\(c_2 = -c_1\\)\n\n\n\n\n\nA 90% CI for the average number of chocolate chips in a Chips Ahoy cookie is: [3.7 chips, 17.2 chips]\nWhat is the correct interpretation?37\n\nThere is a 0.9 prob that the true average number of chips is between 3.7 & 17.2.\n90% of cookies have between 3.7 & 17.2 chips.\nWe are 90% confident that in our sample, the sample average number of chips is between 3.7 and 17.2.\nIn many repeated samples, 90% of sample averages will be between 3.7 and 17.2.\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n\n\n\nA 90% CI for the average number of chocolate chips in a Chips Ahoy cookie: [3.9 chips, \\(\\infty\\))\nWhat is the correct interpretation?38\n\nThere is a 0.9 prob that the true average number of chips is bigger than 3.9\n90% of cookies have more than 3.9 chips\nWe are 90% confident that in our sample, the sample average number of chips is bigger than 3.9.\nIn many repeated samples, 90% of sample averages will be bigger than 3.9\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n\n\n\nConsider a Bayesian posterior interval for \\(\\mu\\) of the form: \\(\\overline{X} \\pm t^*_{n-1} s / \\sqrt{n}\\)\nWhat was the prior on \\(\\mu\\)?39\n\nN(0,0)\nN(\\(\\overline{X}\\),0)\nN(0, 1/0)\nN(\\(\\overline{X}\\),1/0)\nN(1/0, 0)\n\n\n\nSome review questions:\n\nIf we need to find the distribution of a function of one variable (g(X) = X), the easiest route is probably:40\n\nfind the pdf\nfind the cdf\nfind the MGF\nfind the expected value and variance\n\n\n\n\nIf we need to find the distribution of a sum of random variables, the easiest route is probably:41\n\nfind the pdf\nfind the cdf\nfind the MGF\nexpected value and variance\n\n\n\n\nFREQUENTIST: consider the sampling distribution of \\(\\hat{\\theta}.\\) The parameters in the sampling distribution are given by:42\n\nthe data\nthe parameters from the likelihood\nthe prior parameters\nthe statistic\n\\(\\theta\\)\n\n\n\n\nBAYESIAN: consider the posterior distribution of \\(\\theta | \\underline{X}.\\) The parameters in the posterior distribution are a function of:43\n\nthe data\nthe parameters from the likelihood\nthe prior parameters\nthe statistic\n\\(\\theta\\)\n\n\n\n\nA sample of size 8 had a mean of 22.5. It was bootstrapped 1000 times and the mean of the bootstrap distribution was 22.491. The standard deviation of the bootstrap was 2.334. The 95% BS SE confidence interval for the population mean is44\n\n22.491 \\(\\pm\\) z(.975) * 2.334\n22.491 \\(\\pm\\) z(.95) * 2.334\n22.5 \\(\\pm\\) z(.975) * 2.334\n22.5 \\(\\pm\\) z(.95) * 2.334\n22.5 \\(\\pm\\) z(.975) * 2.334 / \\(\\sqrt{8}\\)\n\n\n\n\nWhich is most accurate?45\n\nA BS SE confidence interval\nA bootstrap-t confidence interval\nA bootstrap percentile interval\nA bootstrap BCa interval\n\n\n\n\nWhat is the primary reason to bootstrap a CI (instead of creating a CI from calculus)?46\n\nlarger coverage probabilities\nnarrower intervals\nmore resistant to outliers\ncan be done for statistics with unknown sampling distributions\n\n\n\n\nWhat does the Fisher Information tell us?47\n\nthe variability of the MLE from sample to sample.\nthe bias of the MLE from sample to sample.\nthe variability of the data from sample to sample.\nthe bias of the data from sample to sample.\n\n\n\n\nWhy do we care about the variability of the MLE?48\n\ndetermines whether MOM or MLE is better.\ndetermines whether Bayes’ estimator or MLE is better.\ndetermines how precise the estimator is.\nallows us to do inference (about the population value).\n\n\n\n\nWhy do we care about the sampling distribution of the MLE?49\n\ndetermines whether MOM or MLE is better.\ndetermines whether Bayes’ estimator or MLE is better.\ndetermines how precise the estimator is.\nallows us to do inference (about the population value).\n\n\n\n\nConsider an estimator, \\(\\hat{\\theta}\\), such that \\(E[\\hat{\\theta}] = m(\\theta)\\).\n\\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if:50\n\n\\(m(\\theta)\\) is a function of \\(\\theta\\).\n\\(m(\\theta)\\) is NOT a function of \\(\\theta\\).\n\\(m(\\theta)= \\theta\\).\n\\(m(\\theta)= 0\\).\n\\(m(\\theta)\\) is the expected value of \\(\\hat{\\theta}\\).\n\n\n\n\nIf \\(\\hat{\\theta}\\) is unbiased, \\(m'(\\theta)\\) is51\n\nzero\none\n\\(\\theta\\)\n\\(\\theta^2\\)\nsome other function of \\(\\theta\\), depending on \\(m(\\theta)\\)\n\n\n\n\nThe MLE is52\n\nconsistent\nefficient\nasymptotically normally distributed\nall of the above\n\n\n\n\nWhy don’t we set up our test as: always reject \\(H_0?\\)53\n\ntype I error too high\ntype II error too high\nlevel of sig too high\npower too high\n\n\n\n\nWhy do we care about the distribution of the test statistic?54\n\nBetter estimator\nTo find the rejection region / critical region\nTo minimize the power\nBecause we love the Central Limit Theorem\n\n\n\n\nGiven a statistic T = r(X), how do we find a (good) test?55\n\nMaximize power when \\(H_1\\) is true\nMinimize type I error\nControl type I error\nMinimize type II error\nControl type II error\n\n\n\n\nWe can find the probability of type II error (at a given \\(\\theta \\in \\Omega_1)\\) as56\n\na value of the power curve (at \\(\\theta)\\)\n1 – P(type I error at \\(\\theta)\\)\n\\(\\pi(\\theta | \\delta)\\)\n1- \\(\\pi(\\theta | \\delta)\\)\nwe can’t ever find the probability of a type II error\n\n\n\n\nWhy don’t we use the power function to also control the type II error?57 (We want the power to be big in \\(\\Omega_1\\), so we’d control it by keeping the power from getting too small.)\n\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) does not exist\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) =0\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really big\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) =1\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really small\n\n\n\n\nWith two simple hypotheses, hypothesis testing simplifies because we can now control (i.e., compute):58\n\nthe size of the test.\nthe power of the test.\nthe probability of type I error.\nthe probability of type II error.\na rejection region.\n\n\n\n\nThe likelihood ratio is super awesome because59\n\nit provides the test statistic\nit provides the critical region\nit provides the type I error\nit provides the type II error\nit provides the power\n\n\n\n\nA uniformly most powerful (UMP) test60\n\nhas the highest possible power in \\(\\Omega_1\\).\nhas the lowest possible power in \\(\\Omega_1\\).\nhas the same power over all \\(\\theta \\in \\Omega_1\\).\nhas the highest possible power in \\(\\Omega_1\\) subject to controlling \\(\\alpha(\\delta).\\)\nis a test we try to avoid.\n\n\n\n\nA monotone likelihood ratio statistic is awesome because61\n\nit is the MLE\nit is easy to compute\nits distribution is known\nit is unbiased\nit is monotonic with respect to the likelihood ratio\n\n\n\n\nLikelihood Ratio Test62\n\ngives a statistic for comparing likelihoods\nis always UMP\nworks only with some types of hypotheses\nworks only with hypotheses about one parameter\ngives the distribution of the test statistic\n\n\n\n\nIncreasing sample size63\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nMaking significance level more stringent (\\(\\alpha_0\\) smaller)64\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nA more extreme alternative is true65\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nGiven the situation where \\(H_1: \\mu_1 - \\mu_2 \\ne 0\\) is TRUE. Consider 100 CIs (for \\(\\mu_1 - \\mu_2\\)), the power of the test can be approximated by:66\n\nThe proportion that contain the true mean.\nThe proportion that do not contain the true mean.\nThe proportion that contain zero.\nThe proportion that do not contain zero.\n\n\n\n\nIt is hard to find the power associated with the t-test because:67\n\nthe non-central t-distribution is tricky.\ntwo-sided power is difficult to find.\nwe don’t know the variance.\nthe t-distribution isn’t integrable.\n\n\n\n\nConsider the likelihood ratio statistic:68 \\[\\Lambda(x) = \\frac{\\sup_{\\Omega_1} f(\\underline{x} | \\theta)}{\\sup_{\\Omega_0} f(\\underline{x} | \\theta)}\\]\n\n\\(\\Lambda(x) \\geq 1\\)\n\\(\\Lambda(x) \\leq 1\\)\n\\(\\Lambda(x) \\geq 0\\)\n\\(\\Lambda(x) \\leq 0\\)\nbounds on \\(\\Lambda(x)\\) depend on hypotheses\n\n\n\n\n\n\n\nFootnotes\n\n\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nSort of likely the Green Cab company\n\n↩︎\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\n↩︎\n\nintegrate the joint distribution with respect to Y.\n\n↩︎\n\nIntegrate to 1 (\\(dx\\))\n\n↩︎\n\nhas support on [0,1]\n\n↩︎\n\nprior = marginal & posterior = conditional\n\n↩︎\nBoth (b) \\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12) and (c) \\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12) are incorrect. (b) because the value to the left of the \\(\\sim\\) must be a random variable. (c) because the value to the right of the \\(\\propto\\) must be a function.↩︎\n\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n↩︎\n\ngamma\n\n↩︎\n\nit doesn’t integrate to one.\n\n↩︎\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)↩︎\n\nsome of the above (the Bayes estimator is the posterior mean, it is sensitive to the rest of it.)\n\n↩︎\n\nthe data\n\n↩︎\n\ntheta\n\n↩︎\nwith respect to \\(\\theta\\)↩︎\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))↩︎\n\nhas desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)\n\n↩︎\n\nis often straightforward to compute (it does not always exist, look at Cauchy. it does not always produce estimates inside the parameter space.)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nthe distribution of the statistic in repeated samples\n\n↩︎\n\nthe cdf, the pdf/pmf, and the mgf\n\n↩︎\n\ngives all theoretical moments of the distribution\n\n↩︎\n(e): (a), (c), (d)↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\na little bit more than 1 (dividing by \\(s\\) instead of \\(\\sigma\\) adds variability to the distribution)\n\n↩︎\n\n50 observations in each bootstrap sample\n\n↩︎\n\n1000\n\n↩︎\n\nthe sample statistic\n\n↩︎\n\nResampling with replacement from the original sample. Although I suppose (c) is also true.\n\n↩︎\n\nThe difference between a sampling distribution mean and the actual parameter.\n\n↩︎\n\n-0.009 Bias is what the statistic is (on average) minus the true value. Recall, we are using the data as a proxy for the population, so the “truth” is the data. So in the bootstrap setting, the average is over the bootstrapped values and the true value is the sample mean.\n\n↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(c_2\\) set to infinity\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nN(0,1/0). Or rather, to get the frequentist result, you need the joint improper priors to have \\(\\mu_0 = \\lambda_0 = \\beta_0 = 0\\) and \\(\\alpha_0 = -1/2\\).\n\n↩︎\n\nThe MGF is usually easiest if g is any kind of linear combination. If not, you might need (b) find the cdf. You’ll need to find the cdf to get the pdf, which you might need to identify the distribution. (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nfind the MGF (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nthe parameters from the likelihood\n\n↩︎\n\nthe data and (c) the prior parameters\n\n↩︎\n\n22.5 \\(\\pm\\) z(.975) * 2.334\n\n↩︎\n\nA bootstrap BCa interval (although out of the ones we’ve covered, (b) A bootstrap-t confidence interval is most accurate)\n\n↩︎\n\ncan be done for statistics with unknown sampling distributions\n\n↩︎\n\nthe variability of the MLE from sample to sample.\n\n↩︎\n\ndetermines how precise the estimator is.\n\n↩︎\n\nallows us to do inference (about the population value).\n\n↩︎\n\n\\(m(\\theta)= \\theta\\).\n\n↩︎\n\none\n\n↩︎\n\nall of the above\n\n↩︎\n\ntype I error too high\n\n↩︎\n\nTo find the rejection region / critical region\n\n↩︎\n\nControl type I error\n\n↩︎\n\n1- \\(\\pi(\\theta | \\delta)\\)\n\n↩︎\n\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really small\n\n↩︎\n\nthe power of the test. or (d) the probability of type II error. (they are functions of one another)\n\n↩︎\n\nit provides the test statistic\n\n↩︎\n\nhas the highest possible power in \\(\\Omega_1\\) subject to controlling \\(\\alpha(\\delta).\\)\n\n↩︎\n\nit is monotonic with respect to the likelihood ratio\n\n↩︎\n\ngives the distribution of the test statistic\n\n↩︎\n\nIncreases power (over \\(\\Omega_1\\))\n\n↩︎\n\nDecreases power (over \\(\\Omega_1\\))\n\n↩︎\n\nIncreases your power (over \\(\\Omega_1\\))\n\n↩︎\n\nThe proportion that do not contain zero.\n\n↩︎\n\nthe non-central t-distribution is tricky.\n\n↩︎\n\\(\\Lambda(x) \\geq 1\\)↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Class notes can be found at http://st47s.com/Math152/Notes/.\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nEnjoy the notes / text \nAttend class, review the warm-up and solutions if you have any questions after completing it during class.\nComplete the HW assignment ( pdf &  Rmd linked below),  submit the assignment via Gradescope accessed on the course Canvas site.\nDiscuss the reflection questions  and ethics considerations  (see the  class notes) with your classmates, mentor, and professor.\nThe textbook is Probability and Statistics (P&S), by DeGroot and Schervish (either 3rd or 4th edition is fine). You should be able to find it online.\n\n\n\n\n\n\n\n\n \n  \n    date \n    agenda \n    readings \n    homework \n    handouts \n    warm-ups \n  \n \n\n  \n    Week 1  8.30.22 \n    • logistics + • priors + • posteriors \n    P&S: 7.1-7.2   notes on Bayes \n     HW1 pdf   HW1 Rmd   HW1 turn-in \n    Distribution Sheet \n     WU 1   WU 2 \n  \n  \n    Week 2  9.6.22 \n    • posteriors +  • Bayes estimators \n    P&S: 7.3-7.4   notes on Bayes \n     HW2 pdf   HW2 Rmd   HW2 turn-in \n     \n     WU 3 \n  \n  \n    Week 3  9.13.22 \n    • MLE +  • consistency +  • bias +  • MOM \n    P&S: 7.5-7.6   notes on MLE \n     HW3 pdf   HW3 Rmd   HW3 turn-in \n    Baseball and Bayes \n     WU 4   WU 5 \n  \n  \n    Week 4  9.20.22 \n    • sampling distributions of estimators \n    P&S: 6.1-6.3, 8.1-8.4   notes on sampling distributions \n     HW4 pdf   HW4 Rmd   HW4 turn-in \n    Tanks \n     WU 6   WU 7 \n  \n  \n    Week 5  9.27.22 \n    • bootstrap sampling distributions \n    P&S: 12.6   notes on bootstrap distribution \n     HW5 pdf   HW5 Rmd   HW5 turn-in \n    Bootstrapping \n     WU 8   WU 9 \n  \n  \n    Week 6  2.22.22 \n    • frequentist, Bayesian intervals \n    P&S: 8.5-8.6   notes on intervals \n     HW6 pdf   HW6 Rmd   HW6 turn-in \n     App to compare Freq & Bayes Intervals  2 parameter normal Bayesian \n     WU 10   WU 11 \n  \n  \n    Week 7  10.11.22 \n    review & catch-up \n    exam 1 in class 10.13.22 \n     HW7 pdf   HW7 Rmd   not due, ever \n     see Canvas for sample exam 1 Q & sol \n     \n  \n  \n    Tuesday 10.18.22 \n    Fall Break \n     \n     \n     \n     \n  \n  \n    Week 8  10.20.22 \n    • bootstrap confidence intervals   • earthquake! \n     notes on intervals \n     \n     \n     WU 12 \n  \n  \n    Week 9  10.25.22 \n    • Fisher information + • efficiency + • UMVUE \n    P&S: 8.7-8.8   notes on Fisher information \n     HW8 pdf   HW8 Rmd   HW8 turn-in \n     \n     WU 13   WU 14 \n  \n  \n    Week 10  11.1.22 \n    • hypotehsis testing + • simple hypotheses \n    P&S: 9.1   notes on hypothesis testing structure \n    no HW \n     \n     WU 15   WU 16 \n  \n  \n    Week 11  11.8.22 \n    • Neyman-Pearson \n    P&S: 9.2   notes on Neyman-Pearson \n     HW9 pdf   HW9 Rmd   HW9 turn-in \n     \n     WU 17   WU 18 \n  \n  \n    Week 12  11.15.22 \n    review & catch-up \n    exam 2 in class 11.17.22 \n     HW10 pdf   HW10 Rmd   not due, ever \n     see Canvas for sample exam 2 Q & sol \n     \n  \n  \n    Week 13  11.22.22 \n    • UMP \n    P&S: 9.3   notes on UMP \n     \n     \n     WU 19 \n  \n  \n    Thursday 11.24.22 \n    Thanksgiving \n     \n     \n     \n     \n  \n  \n    Week 14  11.29.22 \n    • two-sided + • t-tests + •  LRT +  goodness of fit \n    P&S: 9.4-9.6   notes on t-tests and LRT \n     \n     \n     \n  \n  \n    Week 15  12.6.22 \n    • Bayes tests + • foundational ideas \n    P&S: 9.8-9.9   notes on Bayes tests and foundational ideas \n     \n     \n     \n  \n  \n    Week 14  4.26.22 \n     \n     \n     \n     \n     \n  \n  \n    Thursday  12.15.22  2-5pm \n    Final Exam  \n     \n     \n     \n     \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW2f22.html",
    "href": "handout/MA152_HW2f22.html",
    "title": "Math 152 - Statistical Theory - Homework 2",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW1f22.html",
    "href": "handout/MA152_HW1f22.html",
    "title": "Math 152 - Statistical Theory - Homework 1",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Wednesday 8:30-10:30 am\nThursday 1:30-3:30 pm\nOr by Appointment\n\n\nWednesday 8-10 pm Estella 2113\n\n\n\n\n\nArtwork by @allison_horst.\n\n\n\n\n\n\n\n\nStatistical Theory is an introduction to statistics for students with a background in probability theory, calculus, and linear algebra. There is no statistics prerequisite for this class. The course will be focused on the theoretical aspects of the material, though there will be some real world applications in class and in the homework assignments. The idea is to have a strong mathematical understanding of the concepts while also understanding how the concepts are applied in the real world.\nAt the completion of this course, students will:\n\nbe able to derive the methods from introductory statistics using tools from mathematics (i.e., calculus and probability).\nbe able to justify the use of a particular method (technical conditions).\nbe able to weigh advantages and disadvantages of different estimation techniques (e.g., bias, variability, resistance to outliers).\nknow to and how to communicate results effectively.\n\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!\n\n\n\n\n\nBy the end of the semester, students will be able to do the following:\n\nunderstand the structure of a linear model, including: simple linear regression, multiple linear regression, ridge regression, Lasso, and splines.\nknow when a linear model is appropriate and what conclusions can be drawn given a particular dataset, including: when are p-values appropriate to use? when is prediction more appropriate? when can or cannot causation be implied?\nuse graphical tools to investigate models associated with the data at hand, including: exploring data graphically, using graphics to understand leverage and influence values, visualizing smooth models.\ncommunicate results effectively.\n\n\n\n\nIn an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates.\n\n\n\n\n\nProbability and Statistics, by DeGroot and Schervish (either 3rd or 4th edition is fine)\n\n\n\n\nDistribution Sheet\nProbability distribution demos: ISLE widgets\nFantastic cheat sheet for Univariate Distributions created by Lindsey Leemis\nBaseball and Bayes\nHow Many Tanks?\nBayesian Distribution of Normal\n\n\n\n\n\n\n\nExam dates\n\n\n\nExam 1 – Thursday, October 13th\nExam 2 – Thursday, November 17th\nFinal Exam – Thursday, December 15th, 2-5pm\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nR Markdown\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using RMarkdown. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)\n\n\n\nThis course uses Canvas as the main learning management system. The Canvas login is http://canvas.pomona.edu/. If you haven’t used Canvas before, I recommend bookmarking Canvas Student Guides and Canvas Student Videos for easy reference to tips and tutorials. If you run into an issue with Canvas, help is available.\n\nFrom anywhere in Canvas, select the Help button, located in the blue Global Navigation menu on the left.\n\nClick on Pomona Service Desk - Canvas Support to report a problem by submitting a service request ticket. Be sure to include “Canvas Issue” in your subject line.\nFor additional assistance, you can click on Ask Your Instructor or simply send me an email.\n\n\nPlease be proactive and reach out for help as soon as possible to resolve the issue you are experiencing.\n\n\n\n\n\n\nThe prerequisites for this class are Probability (Math 151 or equivalent) and completion of the sequence of calculus and linear algebra. We rely heavily on these prerequisites, and students with no background in probability or multivariable calculus will find themselves trying to catch up throughout the semester. You should be familiar with topics such as conditional probabilities and expectations, the Central Limit Theorem, moment generating functions, and probability density functions.\n\n\n\nHomework will be assigned from the text and due every Thursday at midnight. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\n\n\n\nThe midterm exams will include in-class and take-home sections. The first go-around for both parts will be done individually. After the exam is graded, there will be an opportunity to completely revise the entire exam (with help from friends, professor, outside sources, etc.). Part of the grade will be based on the individual contribution, and part of the grade will be based on the revision. More details will be provided during the semester.\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n25% Homework\n50% Midterms\n20% Final Exam\n5% Class Participation"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Statistical Theory",
    "section": "",
    "text": "Clicker Q\nto go with Probability & Statistics by DeGroot and Schervish. Math 152 - Statistical Theory.\n\n\n\nThe Central Limit Theorem (CLT) says:1\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nWhich cab company was involved (see example 2.2 in the notes)?2\n\nVery likely the Blue Cab company\nSort of likely the Blue Cab company\nEqually likely Blue and Green Cab companies\nSort of likely the Green Cab company\nVery likely the Green Cab company\n\n\n\n\nConsider a continuous probability density function (pdf) given by \\(f( x | \\theta ).\\) Which of the following is FALSE:3\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\\(f( x | \\theta )\\) provides info for calculating probabilities of X.\n\\(P(X = x) = 0\\) if X is continuous.\n\\(f( x | \\theta ) = L(\\theta | x)\\) is the likelihood function\n\n\n\n\nTo find a marginal distribution of X from a joint distribution of X & Y you should (assume everything is continuous),4\n\ndifferentiate the joint distribution with respect to X.\ndifferentiate the joint distribution with respect to Y.\nintegrate the joint distribution with respect to X.\nintegrate the joint distribution with respect to Y.\nI have no idea what a marginal distribution is.\n\n\n\n\nA continuous pdf (of a random variable \\(X\\) with parameter \\(\\theta\\)) should5\n\nIntegrate to a constant (\\(dx\\))\nIntegrate to a constant (\\(d\\theta\\))\nIntegrate to 1 (\\(dx\\))\nIntegrate to 1 (\\(d\\theta\\))\nnot need to integrate to anything special.\n\n\n\n\nR / R Studio\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “R”?\n\n\n\n\nIn terms of the R for the homework…\n\nI was able to do the whole thing.\nI understood the code part, but I couldn’t get the Markdown file to compile.\nI didn’t understand the code at all.\nI couldn’t get R or R Studio installed.\nI haven’t tried to work on the homework yet.\n\n\n\n\nA beta distribution6\n\nhas support on [0,1]\nhas parameters \\(\\alpha\\) and \\(\\beta\\) which represent, respectively, the mean and variance\nis discrete\nhas equal mean and variance\nhas equal mean and standard deviation\n\n\n\n\nWhat types of distributions are the following?7\n\nprior = marginal & posterior = joint\nprior = joint & posterior = conditional\nprior = conditional & posterior = joint\nprior = marginal & posterior = conditional\nprior = joint & posterior = marginal\n\n\n\n\nWhich of these are incorrect conclusions?8\n\n\\(\\theta | \\underline{X} \\sim\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12)\n\\(\\xi(\\theta | \\underline{X}) \\propto \\theta^{4-1} (1-\\theta)^{12-1}\\)\n\\(\\xi(\\theta | \\underline{X}) = \\frac{1}{B(4,12)} \\theta^{4-1}(1-\\theta)^{12-1}\\)\n\n\n\n\nWhat is the integrating constant for the pdf, \\(h(w)\\)?9\n\n\\(\\frac{\\Gamma(w+k)}{\\Gamma(w)\\Gamma(k)}\\)\n1/[\\(w^k \\Gamma(k)\\)]\n1 / \\(\\sqrt{2\\pi k^2}\\)\n1/[\\(\\Gamma(k/2)\\)]\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n\n\\[h(w) \\propto w^{k/2-1}e^{-w/2} \\ \\ \\ \\ \\ \\ \\ \\ \\ w>0\\]\n\n\nSuppose the data come from an exponential distribution with a parameter whose prior is given by a gamma distribution. The posterior is known to be conjugate, so its distribution must be in what family?10\n\nexponential\ngamma\nnormal\nbeta\nPoisson\n\n\n\n\nA prior is improper if11\n\nit conveys no real information.\nit isn’t conjugate.\nit doesn’t integrate to one.\nit swears a lot.\nit isn’t on your distribution sheet.\n\n\n\n\nGiven a prior: \\(\\theta \\sim N(\\mu_0, \\nu_0^2)\\)\nAnd a data likelihood: \\(X | \\theta \\sim N(\\theta, \\sigma^2)\\)\nYou collect n data values, what is your best guess of \\(\\theta?\\)12\n\n\\(\\overline{X}\\)\n\\(\\mu_0\\)\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0^2 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)\n\nmedian of \\(N(\\mu_1, \\nu_1^2 = \\frac{\\sigma^2 \\nu_0^2}{\\sigma^2 + n \\nu_0^2})\\)\n\n47\n\n\n\n\n\nThe Bayes estimator is sensitive to13\n\nthe posterior mean\nthe prior mean\nthe sample size\nthe data values\nsome of the above\n\n\n\n\nThe range (output) of the Bayesian MSE includes:14\n\ntheta\nthe data\n\n\n\n\nThe range (output) of the frequentist MSE includes:15\n\ntheta\nthe data\n\n\n\n\nTo find the maximum likelihood estimator, we take the derivative of the likelihood16\n\nwith respect to \\(X\\)\nwith respect to \\(\\underline{X}\\)\nwith respect to \\(\\theta\\)\nwith respect to \\(f\\)\nwith respect to \\(\\ln(f)\\)\n\n\n\n\nConsider an MLE, \\(\\hat{\\theta},\\) and the related log likelihood function \\(L = \\ln(f).\\) \\(\\delta(X)\\) is another estimate of \\(\\theta\\). Which statement is necessarily false:17\n\nL(\\(\\delta(X)\\)) < L(\\(\\theta\\))\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))\nL(\\(\\theta\\)) < L(\\(\\delta(X)\\))\nL(\\(\\delta(X)\\)) < L(\\(\\hat{\\theta}\\))\nL(\\(\\theta\\)) < L(\\(\\hat{\\theta}\\))\n\n\n\n\nThe MLE is popular because it18\n\nmaximizes \\(R^2\\)\nminimizes the sum of squared errors\nhas desirable sampling distribution properties\nmaximizes both the likelihood and the log likelihood\nalways exists\n\n\n\n\nMOM is popular because it:19\n\nhas desirable sampling properties\nis often straightforward to compute\nalways produces values inside the parameter space (e.g., in [0,1] for prob)\nalways exists\n\n\n\n\nThe Central Limit Theorem (CLT) says:20\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nA sampling distribution is21\n\nthe true distribution of the data\nthe estimated distribution of the data\nthe distribution of the population\nthe distribution of the statistic in repeated samples\nthe distribution of the statistic from your one sample of data\n\n\n\n\nThe distribution of a random variable can be uniquely determined by22\n\nthe cdf: F(x)\nthe pdf (pmf): f(x)\nthe moment generating function (mgf), if it exists: \\(\\Psi(t) = E[e^{tX}]\\)\nthe mean and variance of the distribution\nmore than one of the above (which ones??)\n\n\n\n\nA moment generating function23\n\ngives the probability of the RV at any value of X\ngives all theoretical moments of the distribution\ngives all sample moments of the data\ngives the cumulative probability of the RV at any value of X\n\n\n\n\nThe sampling distribution is important because24\n\nit describes the behavior (distribution) of the statistic\nit describes the behavior (distribution) of the data\nit gives us the ability to measure the likelihood of the statistic or more extreme under particular settings (i.e. null)\nit gives us the ability to make inferences about the population parameter\nmore than one of the above (which ones??)\n\n\n\n\nThe following result: \\(\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) allows us to isolate and conduct inference on what parameter?25\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nThe following result: \\(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\) allows us to isolate and conduct inference on what parameter?26\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nWhat would you expect the standard deviation of the t statistic to be?27\n\na little bit less than 1\n1\na little bit more than 1\nunable to tell because it depends on the sample size and the variability of the data\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. What is the sample size of each bootstrap sample?28\n\n50\n1000\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. How many bootstrap statistics will you have?29\n\n50\n1000\n\n\n\n\nThe bootstrap distribution of \\(\\hat{\\theta}\\) is centered around the30\n\npopulation parameter\nsample statistic\nbootstrap statistic\nbootstrap parameter\n\n\n\n\nThe bootstrap theory relies on31\n\nResampling with replacement from the original sample.\nResampling from the original sample, leaving one observation out each time (e.g., cross validation)\nEstimating the population using the sample.\nPermuting the data values within the sample.\n\n\n\n\nBias of a statistic refers to32\n\nThe difference between a statistic and the actual parameter\nWhether or not questions were worded fairly.\nThe difference between a sampling distribution mean and the actual parameter.\n\n\n\n\nThe mean of a sample is 22.5. The mean of 1000 bootstrapped samples is 22.491. The bias of the bootstrap mean is33\n\n-0.009\n-0.0045\n-0.09\n0.009\n0.09\n\n\n\n\nThe following result: \\(\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) allows us to isolate and conduct inference on what parameter?34\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nThe following result: \\(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\\)\nallows us to isolate and conduct inference on what parameter?35\n\n\\(\\overline{X}\\)\n\\(s\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\chi\\)\n\n\n\n\nConsider an asymmetric confidence interval for \\(\\sigma\\) which is derived using:\n\\(P(c_1 \\leq \\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{\\sigma^2} \\leq c_2) = 0.95\\)\nThe resulting 95% interval with the shortest width has:36\n\n\\(c_1\\) and \\(c_2\\) as the .025 & .975 quantiles\n\\(c_1\\) set to zero\n\\(c_2\\) set to infinity\n\\(c_1\\) and \\(c_2\\) as different quantiles than (a) but that contain .95 probability.\nFind \\(c_1\\) and let \\(c_2 = -c_1\\)\n\n\n\n\n\nA 90% CI for the average number of chocolate chips in a Chips Ahoy cookie is: [3.7 chips, 17.2 chips]\nWhat is the correct interpretation?37\n\nThere is a 0.9 prob that the true average number of chips is between 3.7 & 17.2.\n90% of cookies have between 3.7 & 17.2 chips.\nWe are 90% confident that in our sample, the sample average number of chips is between 3.7 and 17.2.\nIn many repeated samples, 90% of sample averages will be between 3.7 and 17.2.\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n\n\n\nA 90% CI for the average number of chocolate chips in a Chips Ahoy cookie: [3.9 chips, \\(\\infty\\))\nWhat is the correct interpretation?38\n\nThere is a 0.9 prob that the true average number of chips is bigger than 3.9\n90% of cookies have more than 3.9 chips\nWe are 90% confident that in our sample, the sample average number of chips is bigger than 3.9.\nIn many repeated samples, 90% of sample averages will be bigger than 3.9\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n\n\n\nConsider a Bayesian posterior interval for \\(\\mu\\) of the form: \\(\\overline{X} \\pm t^*_{n-1} s / \\sqrt{n}\\)\nWhat was the prior on \\(\\mu\\)?39\n\nN(0,0)\nN(\\(\\overline{X}\\),0)\nN(0, 1/0)\nN(\\(\\overline{X}\\),1/0)\nN(1/0, 0)\n\n\n\nSome review questions:\n\nIf we need to find the distribution of a function of one variable (g(X) = X), the easiest route is probably:40\n\nfind the pdf\nfind the cdf\nfind the MGF\nfind the expected value and variance\n\n\n\n\nIf we need to find the distribution of a sum of random variables, the easiest route is probably:41\n\nfind the pdf\nfind the cdf\nfind the MGF\nexpected value and variance\n\n\n\n\nFREQUENTIST: consider the sampling distribution of \\(\\hat{\\theta}.\\) The parameters in the sampling distribution are given by:42\n\nthe data\nthe parameters from the likelihood\nthe prior parameters\nthe statistic\n\\(\\theta\\)\n\n\n\n\nBAYESIAN: consider the posterior distribution of \\(\\theta | \\underline{X}.\\) The parameters in the posterior distribution are a function of:43\n\nthe data\nthe parameters from the likelihood\nthe prior parameters\nthe statistic\n\\(\\theta\\)\n\n\n\n\nA sample of size 8 had a mean of 22.5. It was bootstrapped 1000 times and the mean of the bootstrap distribution was 22.491. The standard deviation of the bootstrap was 2.334. The 95% BS SE confidence interval for the population mean is44\n\n22.491 \\(\\pm\\) z(.975) * 2.334\n22.491 \\(\\pm\\) z(.95) * 2.334\n22.5 \\(\\pm\\) z(.975) * 2.334\n22.5 \\(\\pm\\) z(.95) * 2.334\n22.5 \\(\\pm\\) z(.975) * 2.334 / \\(\\sqrt{8}\\)\n\n\n\n\nWhich is most accurate?45\n\nA BS SE confidence interval\nA bootstrap-t confidence interval\nA bootstrap percentile interval\nA bootstrap BCa interval\n\n\n\n\nWhat is the primary reason to bootstrap a CI (instead of creating a CI from calculus)?46\n\nlarger coverage probabilities\nnarrower intervals\nmore resistant to outliers\ncan be done for statistics with unknown sampling distributions\n\n\n\n\nWhat does the Fisher Information tell us?47\n\nthe variability of the MLE from sample to sample.\nthe bias of the MLE from sample to sample.\nthe variability of the data from sample to sample.\nthe bias of the data from sample to sample.\n\n\n\n\nWhy do we care about the variability of the MLE?48\n\ndetermines whether MOM or MLE is better.\ndetermines whether Bayes’ estimator or MLE is better.\ndetermines how precise the estimator is.\nallows us to do inference (about the population value).\n\n\n\n\nWhy do we care about the sampling distribution of the MLE?49\n\ndetermines whether MOM or MLE is better.\ndetermines whether Bayes’ estimator or MLE is better.\ndetermines how precise the estimator is.\nallows us to do inference (about the population value).\n\n\n\n\nConsider an estimator, \\(\\hat{\\theta}\\), such that \\(E[\\hat{\\theta}] = m(\\theta)\\).\n\\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if:50\n\n\\(m(\\theta)\\) is a function of \\(\\theta\\).\n\\(m(\\theta)\\) is NOT a function of \\(\\theta\\).\n\\(m(\\theta)= \\theta\\).\n\\(m(\\theta)= 0\\).\n\\(m(\\theta)\\) is the expected value of \\(\\hat{\\theta}\\).\n\n\n\n\nIf \\(\\hat{\\theta}\\) is unbiased, \\(m'(\\theta)\\) is51\n\nzero\none\n\\(\\theta\\)\n\\(\\theta^2\\)\nsome other function of \\(\\theta\\), depending on \\(m(\\theta)\\)\n\n\n\n\nThe MLE is52\n\nconsistent\nefficient\nasymptotically normally distributed\nall of the above\n\n\n\n\nWhy don’t we set up our test as: always reject \\(H_0?\\)53\n\ntype I error too high\ntype II error too high\nlevel of sig too high\npower too high\n\n\n\n\nWhy do we care about the distribution of the test statistic?54\n\nBetter estimator\nTo find the rejection region / critical region\nTo minimize the power\nBecause we love the Central Limit Theorem\n\n\n\n\nGiven a statistic T = r(X), how do we find a (good) test?55\n\nMaximize power when \\(H_1\\) is true\nMinimize type I error\nControl type I error\nMinimize type II error\nControl type II error\n\n\n\n\nWe can find the probability of type II error (at a given \\(\\theta \\in \\Omega_1)\\) as56\n\na value of the power curve (at \\(\\theta)\\)\n1 – P(type I error at \\(\\theta)\\)\n\\(\\pi(\\theta | \\delta)\\)\n1- \\(\\pi(\\theta | \\delta)\\)\nwe can’t ever find the probability of a type II error\n\n\n\n\nWhy don’t we use the power function to also control the type II error?57 (We want the power to be big in \\(\\Omega_1\\), so we’d control it by keeping the power from getting too small.)\n\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) does not exist\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) =0\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really big\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) =1\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really small\n\n\n\n\nWith two simple hypotheses, hypothesis testing simplifies because we can now control (i.e., compute):58\n\nthe size of the test.\nthe power of the test.\nthe probability of type I error.\nthe probability of type II error.\na rejection region.\n\n\n\n\nThe likelihood ratio is super awesome because59\n\nit provides the test statistic\nit provides the critical region\nit provides the type I error\nit provides the type II error\nit provides the power\n\n\n\n\nA uniformly most powerful (UMP) test60\n\nhas the highest possible power in \\(\\Omega_1\\).\nhas the lowest possible power in \\(\\Omega_1\\).\nhas the same power over all \\(\\theta \\in \\Omega_1\\).\nhas the highest possible power in \\(\\Omega_1\\) subject to controlling \\(\\alpha(\\delta).\\)\nis a test we try to avoid.\n\n\n\n\nA monotone likelihood ratio statistic is awesome because61\n\nit is the MLE\nit is easy to compute\nits distribution is known\nit is unbiased\nit is monotonic with respect to the likelihood ratio\n\n\n\n\nLikelihood Ratio Test62\n\ngives a statistic for comparing likelihoods\nis always UMP\nworks only with some types of hypotheses\nworks only with hypotheses about one parameter\ngives the distribution of the test statistic\n\n\n\n\nIncreasing sample size63\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nMaking significance level more stringent (\\(\\alpha_0\\) smaller)64\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nA more extreme alternative is true65\n\nIncreases power (over \\(\\Omega_1\\))\nDecreases power (over \\(\\Omega_1\\))\n\n\n\n\nGiven the situation where \\(H_1: \\mu_1 - \\mu_2 \\ne 0\\) is TRUE. Consider 100 CIs (for \\(\\mu_1 - \\mu_2\\)), the power of the test can be approximated by:66\n\nThe proportion that contain the true mean.\nThe proportion that do not contain the true mean.\nThe proportion that contain zero.\nThe proportion that do not contain zero.\n\n\n\n\nIt is hard to find the power associated with the t-test because:67\n\nthe non-central t-distribution is tricky.\ntwo-sided power is difficult to find.\nwe don’t know the variance.\nthe t-distribution isn’t integrable.\n\n\n\n\nConsider the likelihood ratio statistic:68 \\[\\Lambda(x) = \\frac{\\sup_{\\Omega_1} f(\\underline{x} | \\theta)}{\\sup_{\\Omega_0} f(\\underline{x} | \\theta)}\\]\n\n\\(\\Lambda(x) \\geq 1\\)\n\\(\\Lambda(x) \\leq 1\\)\n\\(\\Lambda(x) \\geq 0\\)\n\\(\\Lambda(x) \\leq 0\\)\nbounds on \\(\\Lambda(x)\\) depend on hypotheses\n\n\n\n\n\n\n\nFootnotes\n\n\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nSort of likely the Green Cab company\n\n↩︎\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\n↩︎\n\nintegrate the joint distribution with respect to Y.\n\n↩︎\n\nIntegrate to 1 (\\(dx\\))\n\n↩︎\n\nhas support on [0,1]\n\n↩︎\n\nprior = marginal & posterior = conditional\n\n↩︎\nBoth (b) \\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12) and (c) \\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12) are incorrect. (b) because the value to the left of the \\(\\sim\\) must be a random variable. (c) because the value to the right of the \\(\\propto\\) must be a function.↩︎\n\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n↩︎\n\ngamma\n\n↩︎\n\nit doesn’t integrate to one.\n\n↩︎\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)↩︎\n\nsome of the above (the Bayes estimator is the posterior mean, it is sensitive to the rest of it.)\n\n↩︎\n\nthe data\n\n↩︎\n\ntheta\n\n↩︎\nwith respect to \\(\\theta\\)↩︎\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))↩︎\n\nhas desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)\n\n↩︎\n\nis often straightforward to compute (it does not always exist, look at Cauchy. it does not always produce estimates inside the parameter space.)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nthe distribution of the statistic in repeated samples\n\n↩︎\n\nthe cdf, the pdf/pmf, and the mgf\n\n↩︎\n\ngives all theoretical moments of the distribution\n\n↩︎\n(e): (a), (c), (d)↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\na little bit more than 1 (dividing by \\(s\\) instead of \\(\\sigma\\) adds variability to the distribution)\n\n↩︎\n\n50 observations in each bootstrap sample\n\n↩︎\n\n1000\n\n↩︎\n\nthe sample statistic\n\n↩︎\n\nResampling with replacement from the original sample. Although I suppose (c) is also true.\n\n↩︎\n\nThe difference between a sampling distribution mean and the actual parameter.\n\n↩︎\n\n-0.009 Bias is what the statistic is (on average) minus the true value. Recall, we are using the data as a proxy for the population, so the “truth” is the data. So in the bootstrap setting, the average is over the bootstrapped values and the true value is the sample mean.\n\n↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(c_2\\) set to infinity\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nN(0,1/0). Or rather, to get the frequentist result, you need the joint improper priors to have \\(\\mu_0 = \\lambda_0 = \\beta_0 = 0\\) and \\(\\alpha_0 = -1/2\\).\n\n↩︎\n\nThe MGF is usually easiest if g is any kind of linear combination. If not, you might need (b) find the cdf. You’ll need to find the cdf to get the pdf, which you might need to identify the distribution. (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nfind the MGF (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nthe parameters from the likelihood\n\n↩︎\n\nthe data and (c) the prior parameters\n\n↩︎\n\n22.5 \\(\\pm\\) z(.975) * 2.334\n\n↩︎\n\nA bootstrap BCa interval (although out of the ones we’ve covered, (b) A bootstrap-t confidence interval is most accurate)\n\n↩︎\n\ncan be done for statistics with unknown sampling distributions\n\n↩︎\n\nthe variability of the MLE from sample to sample.\n\n↩︎\n\ndetermines how precise the estimator is.\n\n↩︎\n\nallows us to do inference (about the population value).\n\n↩︎\n\n\\(m(\\theta)= \\theta\\).\n\n↩︎\n\none\n\n↩︎\n\nall of the above\n\n↩︎\n\ntype I error too high\n\n↩︎\n\nTo find the rejection region / critical region\n\n↩︎\n\nControl type I error\n\n↩︎\n\n1- \\(\\pi(\\theta | \\delta)\\)\n\n↩︎\n\n\\(\\inf_{\\theta \\in \\Omega_1} \\pi(\\theta | \\delta)\\) = always really small\n\n↩︎\n\nthe power of the test. or (d) the probability of type II error. (they are functions of one another)\n\n↩︎\n\nit provides the test statistic\n\n↩︎\n\nhas the highest possible power in \\(\\Omega_1\\) subject to controlling \\(\\alpha(\\delta).\\)\n\n↩︎\n\nit is monotonic with respect to the likelihood ratio\n\n↩︎\n\ngives the distribution of the test statistic\n\n↩︎\n\nIncreases power (over \\(\\Omega_1\\))\n\n↩︎\n\nDecreases power (over \\(\\Omega_1\\))\n\n↩︎\n\nIncreases your power (over \\(\\Omega_1\\))\n\n↩︎\n\nThe proportion that do not contain zero.\n\n↩︎\n\nthe non-central t-distribution is tricky.\n\n↩︎\n\\(\\Lambda(x) \\geq 1\\)↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW3f22.html",
    "href": "handout/MA152_HW3f22.html",
    "title": "Math 152 - Statistical Theory - Homework 3",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW4f22.html",
    "href": "handout/MA152_HW4f22.html",
    "title": "Math 152 - Statistical Theory - Homework 4",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW8f22.html",
    "href": "handout/MA152_HW8f22.html",
    "title": "Math 152 - Statistical Theory - Homework 8",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW6f22.html",
    "href": "handout/MA152_HW6f22.html",
    "title": "Math 152 - Statistical Theory - Homework 6",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW9f22.html",
    "href": "handout/MA152_HW9f22.html",
    "title": "Math 152 - Statistical Theory - Homework 9",
    "section": "",
    "text": "You should work to turn in assignments that are clear, communicative, and concise. Part of what you need to do is not print pages and pages of output. Additionally, you should remove these exact sentences and the information about HW scoring below.\nClick on the Knit to PDF icon at the top of R Studio to run the R code and create a PDF document simultaneously. [PDF will only work if either (1) you are using R on the network, or (2) you have LaTeX installed on your computer. Lightweight LaTeX installation here: https://yihui.name/tinytex/]\n\nEither use the college’s RStudio server (https://rstudio.pomona.edu/) or install R and R Studio on to your personal computer. See: https://m152-stat-theory.netlify.app/syllabus.html for resources.\n\n\n\n\n\n\n\n\nIn this assignment, the fun will include:\n\ncalculating Fisher information\nsetting up hypotheses\ncalculationg power and error rates.\n\nBook problems\n\nFeel free to do the book problems with a pencil or in LaTeX (RMarkdown supports writing mathematics using LaTeX).\n\nIf you use a pencil, you can take a picture of the problem(s), and include the image(s) using (remove the tick marks to make it work):\n\n![](myimage.jpeg)\n\nNote that myimage.jpeg needs to live in the same folder as the relevant .Rmd file (maybe you called the folder “math 152 hw” and put it on your desktop?)\nSaving as jpg, jpeg, png, or pdf should work, but make sure to specify the exact name of the file.\nIf you have the 3rd edition of the book, the problems will be the same unless they don’t exist – that is, the 4th edition added problems but didn’t change the order of them. Ask me if you want to see the 4th edition problems.\n\n\n\n\n\n\n\nDescribe one thing you learned (not from lecture, maybe from working in pairs during class) from a member of the class (student, mentor, professor) – it could be: content, logistical help, background material, R information, etc. 1-3 sentences.\n\n\n\nSuppose that a random variable has the normal distribution with mean 0 and unknown standard deviation \\(\\sigma > 0\\). Find the Fisher information \\(I(\\sigma)\\) in \\(X\\).\n\n\n\nSuppose that a random variable X has the normal distribution with mean 0 and unknown variance \\(\\sigma^2 > 0\\). Find the Fisher information \\(I(\\sigma^2)\\) in \\(X\\). Note that in this exercise the variance \\(\\sigma^2\\) is regarded as the parameter, whereas in Exercise 8.8.4 the standard deviation \\(\\sigma\\) is regarded as the parameter.\nAlso, show that the unbiased estimator of \\(\\sigma^2\\) (\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\overline{X})^2\\)) is not efficient.\nNote to self: there is no simple way to get from problem 8.8.4 to 8.8.5. That is, we know the MLE of a function is that function of the MLE. Not true for information.\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) form a random sample from the Bernoulli distribution with unknown parameter \\(p\\), and the prior pdf of \\(p\\) is a positive and differentiable function over the interval \\(0 < p < 1\\). Suppose, furthermore, that \\(n\\) is large, the observed values of \\(X_1, \\ldots, X_n\\) are \\(x_1, \\ldots, x_n\\), and \\(0 < \\overline{x} < 1\\). Show that the posterior distribution of \\(p\\) will be approximately a normal distribution with mean \\(\\overline{x}\\) and variance \\(\\overline{x}(1- \\overline{x})/n\\).\nNote to self: this is a Bayesian problem. See the connection between Fisher Information and asymptotic / approximate Bayesian distributions in the text.\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) form a random sample from a distribution for which the pdf is as follows:\n\\[f_x(x|\\theta) = \\begin{cases} \\theta x^{\\theta-1} & 0 < x < 1 \\\\ 0 & \\mbox{otherwise} \\end{cases}\\]\nwhere the value of \\(\\theta\\) is unknown (\\(\\theta > 0\\)). Determine the asymptotic distribution of the MLE of \\(\\theta\\).\n\n\n\nLet \\(X\\) have the exponential distribution with parameter \\(\\beta\\). Suppose that we wish to test the hypotheses \\(H_0: \\beta \\geq 1\\) versus \\(H_1: \\beta < 1.\\)\n\\[f(x|\\beta) = \\beta e^{-\\beta x}  \\ \\ \\ \\ x > 0\\]\nConsider the test procedure \\(\\delta\\) that rejects \\(H_0\\) if \\(X \\geq 1.\\)\n\nDetermine the power function of the test.\nCompute the size of the test.\n\n\n\n\nSuppose that a single observation \\(X\\) is to be taken from the uniform distribution on the interval \\([\\theta - \\frac{1}{2}, \\theta + \\frac{1}{2}]\\), and suppose that the following hypotheses are to be tested:\n\\(H_0: \\theta \\leq 3,\\)\n\\(H_1: \\theta \\geq 4.\\)\nConstruct a test procedure \\(\\delta\\) for which the power function has the following values: \\(\\pi(\\theta | \\delta) = 0\\) for \\(\\theta \\leq 3\\) and \\(\\pi(\\theta | \\delta) = 1\\) for \\(\\theta \\geq 4\\).\n\n\n\nAssume that \\(X_1,\\ldots, X_n\\) are i.i.d. with the normal distribution that has mean \\(\\mu\\) and variance 1. Suppose that we wish to test the hypotheses:\n\\(H_0: \\mu \\geq \\mu_0,\\)\n\\(H_1: \\mu < \\mu_0.\\)\nFind a test statistic \\(T\\) such that, for every \\(c\\), the test \\(\\delta_c\\) that rejects \\(H_0\\) when \\(T \\geq c\\) has power function \\(\\pi(\\mu|\\delta_c)\\) that is decreasing in \\(\\mu\\).\n\n\n\nWe are going to study almost, but not exactly, the same model as from the exam. The model for this problem is normal with mean and standard deviation both \\(\\theta\\) (i.e., variance \\(\\theta^2\\), not \\(\\theta\\) as in the example from the exam). Therefore, we know \\(\\theta \\geq 0\\).\nThe results from class about properties of MLEs are asymptotic. What happens in small samples?\n\nThe estimators of \\(\\theta\\) we wish to compare are:\n\nthe sample median\nthe sample mean\nthe sample standard deviation times the sign of the sample mean\nthe MLE\n\n\nThe MLE of \\(\\theta\\) is \\[\\hat{\\theta} =  - \\overline{x}/2 + \\sqrt{\\bigg(\\sum x_i^2\\bigg)/n + \\overline{x}^2/4}\\]. Show (with pencil / LaTeX) that the Fisher Information in \\(\\theta\\) is \\(3/\\theta^2\\).\nUse a simulation to compare the four estimators above with respect to bias, variance, and MSE. Answer the following questions in your comparison:\n\nWhich estimator is (empirically) least biased?\nWhich estimator has lowest empirical variability? Do any of the estimators reach the CRLB (assume unbiasedness)?\nWhich estimator has lowest empirical MSE?\nAre you comfortable with the idea of using a normal distribution to describe the sampling distribution for any/all of the estimators? Explain.\nWhich estimator would you recommend for the given setting? Explain.\n\n\n\n# Use sample size n = 15. keep this set-up in the first 3 lines\nn.obs <- 15\nn.samps <- 10000\ntheta <- exp(1)\n\n\nmeans <- numeric(n.samps)\nmedians <- numeric(n.samps)\nsds <- numeric(n.samps)\nMLEs <- numeric(n.samps)\n\n\nfor (i in 1:n.samps){\n  # generate some data from the given model\n  # means[i] <- mean(the data you generated)\n  # etc.\n}\n\nYou can write alternative code for calculating relevant characteristics of the distribution and displaying it, but I choose to put it together in a tidy framework like this:\nNote: in the code below I’ve created smoothed histograms (called density plots) so as to plot the empirical distributions on top of one another.\nlibrary(tidyverse)\n\nest.info <- data.frame(value = c(means , ___, ...), \n                       type = c(rep(\"mean\", n.samps), ____, ...) )\n\nest.info %>%\n  group_by(type) %>%\n  summarize(est.bias = mean(value) - theta, est.mean = mean(value), \n            est.var = var(value), est.sd = sd(value)) %>%\n  mutate(est.mse = est.var + est.bias^2)\n\n\nest.info %>%\n  ggplot(aes(x=value, color = type)) + geom_density() +\n  geom_vline(xintercept = exp(1))"
  },
  {
    "objectID": "handout/MA152_HW5f22.html",
    "href": "handout/MA152_HW5f22.html",
    "title": "Math 152 - Statistical Theory - Homework 5",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW7f22.html",
    "href": "handout/MA152_HW7f22.html",
    "title": "Math 152 - Statistical Theory - Homework 7",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW10f22.html",
    "href": "handout/MA152_HW10f22.html",
    "title": "Math 152 - Statistical Theory - Homework 10",
    "section": "",
    "text": "You should work to turn in assignments that are clear, communicative, and concise. Part of what you need to do is not print pages and pages of output. Additionally, you should remove these exact sentences and the information about HW scoring below.\nClick on the Knit to PDF icon at the top of R Studio to run the R code and create a PDF document simultaneously. [PDF will only work if either (1) you are using R on the network, or (2) you have LaTeX installed on your computer. Lightweight LaTeX installation here: https://yihui.name/tinytex/]\n\nEither use the college’s RStudio server (https://rstudio.pomona.edu/) or install R and R Studio on to your personal computer. See: https://m152-stat-theory.netlify.app/syllabus.html for resources.\n\n\n\n\n\n\n\n\nIn this assignment, the fun will include:\n\nsize, power, and p-values\nsimple vs simple test\nmost powerful tests\n\nBook problems\n\nFeel free to do the book problems with a pencil or in LaTeX (RMarkdown supports writing mathematics using LaTeX).\n\nIf you use a pencil, you can take a picture of the problem(s), and include the image(s) using (remove the tick marks to make it work):\n\n![](myimage.jpeg)\n\nNote that myimage.jpeg needs to live in the same folder as the relevant .Rmd file (maybe you called the folder “math 152 hw” and put it on your desktop?)\nSaving as jpg, jpeg, png, or pdf should work, but make sure to specify the exact name of the file.\nIf you have the 3rd edition of the book, the problems will be the same unless they don’t exist – that is, the 4th edition added problems but didn’t change the order of them. Ask me if you want to see the 4th edition problems.\n\n\n\n\n\n\n\nDescribe one thing you learned (not from lecture, maybe from working in pairs during class) from a member of the class (student, mentor, professor) – it could be: content, logistical help, background material, R information, etc. 1-3 sentences.\n\n\n\nLet \\(X\\) have the Poisson distribution with mean \\(\\theta\\). Suppose that we wish to test the hypotheses:\n\\(H_0: \\theta \\leq 1.0,\\)\n\\(H_1: \\theta >1.0.\\)\nLet \\(\\delta_c\\) be the test that rejects \\(H_0\\) if \\(X \\geq c\\). Find \\(c\\) to make the size of \\(\\delta_c\\) as close as possible to 0.1 without being larger than 0.1.\nNote: in R use ppois().\n\n\n\nLet \\(X_1, \\ldots , X_n\\) be i.i.d. with the exponential distribution with parameter \\(\\theta\\). Suppose that we wish to test the hypotheses:\n\\(H_0: \\theta \\geq \\theta_0,\\)\n\\(H_1: \\theta < \\theta_0.\\)\nLet \\(X = \\sum_{i=1}^n X_i\\). Let \\(\\delta_c\\) be the test that rejects \\(H_0\\) if \\(X \\geq c.\\)\n\nShow that \\(\\pi(\\theta | \\delta_c)\\) is a decreasing function of \\(\\theta\\).\nFind \\(c\\) in order to make \\(\\delta_c\\) have size \\(\\alpha_0\\).\nLet \\(\\theta_0 = 2\\), \\(n = 1\\), and \\(\\alpha_0 = 0.1\\). Find the precise form of the test \\(\\delta_c\\) and sketch its power function.\n\nNote: \\(\\theta * \\sum(x_i) \\sim\\) Gamma(n,1)\n\n\n\nLet \\(X\\) have the uniform distribution on the interval \\([0, \\theta]\\), and suppose that we wish to test the hypotheses\n\\(H_0: \\theta \\leq 1,\\)\n\\(H_1: \\theta > 1.\\)\nWe shall consider test procedures of the form “reject \\(H_0\\) if \\(X \\geq c\\).” For each possible value \\(x\\) of \\(X\\), find the p-value if \\(X = x\\) is observed (that is, find the p-value as a function of \\(x\\)).\n\n\n\nConsider two pdfs \\(f_0(x)\\) and \\(f_1(x)\\) that are defined as follows:\n\\[f_0(x) = \\begin{cases}\n1 & \\mbox{ for } 0 \\leq x \\leq 1,\\\\\n0 & \\mbox{ otherwise},\\\\\n\\end{cases}\\]\nand \\[f_1(x) = \\begin{cases}\n2x & \\mbox{ for } 0 \\leq x \\leq 1,\\\\\n0 & \\mbox{ otherwise}.\\\\\n\\end{cases}\\]\nSuppose that a single observation \\(X\\) is taken from a distribution for which the pdf \\(f(x)\\) is either \\(f_0(x)\\) or \\(f_1(x)\\), and the following simple hypotheses are to be tested:\n\\(H_0: f(x) = f_0(x),\\)\n\\(H_1: f(x) = f_1(x).\\)\n\nDescribe a test procedure for which the value of \\(\\alpha(\\delta) + 2 \\beta(\\delta)\\) is a minimum.\nDetermine the minimum value of \\(\\alpha(\\delta) + 2 \\beta(\\delta)\\) attained by that test procedure.\n\nConvince yourself that the theorems in 9.2 hold.\n\n\n\nConsider again the conditions of Exercise 9.2.2, but suppose now that it is desired to find a test procedure for which \\(\\alpha(\\delta) \\leq 0.1\\) and \\(\\beta(\\delta)\\) is a minimum.\n\nDescribe the test procedure.\nDetermine the minimum value of \\(\\beta(\\delta)\\) attained by the test procedure.\n\nConvince yourself that the theorems in 9.2 hold.\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) form a random sample from the normal distribution with known mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\), and the following simple hypotheses are to be tested:\n\\(H_0: \\sigma^2 = 2\\),\n\\(H_1: \\sigma^2 = 3\\).\n\nShow that among all test procedures for which \\(\\alpha(\\delta) \\leq 0.05\\), the value of \\(\\beta(\\delta)\\) is minimized by a test procedure that rejects \\(H_0\\) when \\(\\sum^n_{i=1}(X_i - \\mu)^2 > c\\).\n\nFor \\(n = 8\\), find the value of the constant \\(c\\) that appears in part a.\n\n\n\n\nSuppose that a single observation X is taken from the uniform distribution on the interval \\([0, \\theta]\\), where the value of \\(\\theta\\) is unknown, and the following simple hypotheses are to be tested:\n\\(H_0: \\theta = 1\\),\n\\(H_1: \\theta = 2\\).\n\nShow that there exists a test procedure for which \\(\\alpha(\\delta) = 0\\) and \\(\\beta(\\delta) < 1\\).\nAmong all test procedures for which \\(\\alpha(\\delta) = 0\\), find the one for which \\(\\beta(\\delta)\\) is a minimum.\n\n\n\n\nMost people are right-handed and even the right eye is dominant for most people. Molecular biologists have suggested that late-stage human embryos tend to turn their heads to the right. German biopsychologist Onur Güntürkün conjectured that this tendency to turn to the right manifests itself in other ways as well, so he studied kissing couples to see if both people tended to lean to their right more often than to their left (and if so, how strong the tendency is). He and his researchers observed couples from age 13 to 70 in public places such as airports, train stations, beaches, and parks in the United States, Germany, and Turkey. They were careful not to include couples who were holding objects such as luggage that might have affected which direction they turned. In total, 124 kissing pairs were observed with 80 couples leaning right (Nature, 2003). [Taken from Rossman & Chance, ISCAM]\nGüntürkün wanted to test the belief that the probability of kissing to the right is 3/4; he thinks it is probably less than 3/4.\n\nUsing the binomial distribution (not the CLT), find the rejection region for this test given a level of significance of 0.05. You can use trial and error (with pbinom()) or the quantile function for the binomial (qbinom()) to come up with your test.\n\n\n# for X ~ Bin(size, prob)\nsize=124\nprob=.75\nq=2\np=.1\npbinom(q, size, prob)     # gives P(X < = q)\n\n[1] 1.525648e-70\n\ndbinom(q, size, prob)     # gives P(X=q)\n\n[1] 1.517401e-70\n\nqbinom(p, size, prob)     # gives the cutoff for a given probability, p\n\n[1] 87\n\n\n\nWhat is the size of your test?\nCalculate and plot the power function over all possible values of \\(\\theta\\). Do you think your test seems particularly powerful? Explain. (Note: you’ll need to change the code below to say eval = TRUE and also actually write out the power function.)\n\n\nall.theta <-  seq(0,1,.001)\nall.power <-  #somefunction of all.theta and n (size)#\nplot(all.theta, all.power, xlab=\"possible theta values\", ylab=\"power function\")\n\n\nGiven the data (80 couples kissed right), what is the p-value of the test?"
  }
]