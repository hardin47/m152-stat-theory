[
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Statistical Theory",
    "section": "",
    "text": "Footnotes\n\n\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nSort of likely the Green Cab company\n\n↩︎\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\n↩︎\n\nintegrate the joint distribution with respect to Y.\n\n↩︎\n\nIntegrate to 1 (\\(dx\\))\n\n↩︎\n\nhas support on [0,1]\n\n↩︎\n\nprior = marginal & posterior = conditional\n\n↩︎\nBoth (b) \\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12) and (c) \\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12) are incorrect. (b) because the value to the left of the \\(\\sim\\) must be a random variable. (c) because the value to the right of the \\(\\propto\\) must be a function.↩︎\n\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n↩︎\n\ngamma\n\n↩︎\n\nit doesn’t integrate to one.\n\n↩︎\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)↩︎\n\nsome of the above (the Bayes estimator is the posterior mean, it is sensitive to the rest of it.)\n\n↩︎\n\nthe data\n\n↩︎\n\ntheta\n\n↩︎\nwith respect to \\(\\theta\\)↩︎\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))↩︎\n\nhas desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)\n\n↩︎\n\nis often straightforward to compute (it does not always exist, look at Cauchy. it does not always produce estimates inside the parameter space.)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nthe distribution of the statistic in repeated samples\n\n↩︎\n\nthe cdf, the pdf/pmf, and the mgf\n\n↩︎\n\ngives all theoretical moments of the distribution\n\n↩︎\n(e): (a), (c), (d)↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\na little bit more than 1 (dividing by \\(s\\) instead of \\(\\sigma\\) adds variability to the distribution)\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nEnjoy the notes / text \nAttend class, review the warm-up and solutions if you have any questions after completing it during class.\nComplete the HW assignment ( pdf &  Rmd linked below),  submit the assignment via Gradescope accessed on the course Canvas site.\nDiscuss the reflection questions  and ethics considerations  (see the  class notes) with your classmates, mentor, and professor.\nThe textbook is Probability and Statistics (P&S), by DeGroot and Schervish (either 3rd or 4th edition is fine). You should be able to find it online.\n\n\n\n\n\n\n\n\n \n  \n    date \n    agenda \n    readings \n    homework \n    handouts \n    warm-ups \n  \n \n\n  \n    Week 1  8.30.22 \n    • logistics + • priors + • posteriors \n    P&S: 7.1-7.2   notes on Bayes \n     HW1 pdf   HW1 Rmd   HW1 turn-in \n    Distribution Sheet \n     WU 1   WU 2 \n  \n  \n    Week 2  9.6.22 \n    • posteriors +  • Bayes estimators \n    P&S: 7.3-7.4   notes on Bayes \n     HW2 pdf   HW2 Rmd   HW2 turn-in \n     \n     WU 3 \n  \n  \n    Week 3  9.13.22 \n    • MLE +  • consistency +  • bias +  • MOM \n    P&S: 7.5-7.6   notes on MLE \n     HW3 pdf   HW3 Rmd   HW3 turn-in \n    Baseball and Bayes \n     WU 4   WU 5 \n  \n  \n    Week 4  9.20.22 \n    • sampling distributions of estimators \n    P&S: 6.1-6.3, 8.1-8.4   notes on sampling distributions \n     HW4 pdf   HW4 Rmd   HW4 turn-in \n    Tanks \n     WU 6   WU 7 \n  \n  \n    Week 5  9.27.22 \n    • bootstrap sampling distributions \n    P&S: 12.6   notes on bootstrap distribution \n     HW5 pdf   HW5 Rmd   HW5 turn-in \n    Bootstrapping \n     WU 8   WU 9 \n  \n  \n    Week 6  2.22.22 \n    • frequentist, Bayesian intervals \n    P&S: 8.5-8.6   notes on intervals \n     HW6 pdf   HW6 Rmd   HW6 turn-in \n     App to compare Freq & Bayes Intervals  2 parameter normal Bayesian \n     WU 10   WU 11 \n  \n  \n    Week 7  10.11.22 \n    review & catch-up \n    exam 1 in class 10.13.22 \n     HW7 pdf   HW7 Rmd  not due, ever \n     see Canvas for sample exam Q & sol \n     \n  \n  \n    Tuesday 10.18.22 \n    Fall Break \n     \n     \n     \n     \n  \n  \n    Week 8  10.20.22 \n    • bootstrap confidence intervals   • earthquake! \n     notes on intervals \n     \n     \n     WU 12 \n  \n  \n    Week 9  10.25.22 \n    • Fisher information + • efficiency + • UMVUE \n    P&S: 8.7-8.8   \n     HW8 pdf   HW8 Rmd   HW8 turn-in \n     \n     WU 13   WU 14 \n  \n  \n    Week 10  11.1.22 \n    • hypotehsis testing + • simple hypotheses \n    P&S: 9.1   \n     \n     \n     \n  \n  \n    Week 11  11.8.22 \n    • Neyman-Pearson \n    P&S: 9.2   \n     \n     \n     \n  \n  \n    Week 12  11.15.22 \n    review & catch-up \n    exam 2 in class 11.17.22 \n     \n     \n     \n  \n  \n    Week 13  11.22.22 \n    • UMP \n    \\makecell[c]{P&S: 9.3  } \n     \n     \n     \n  \n  \n    Thursday 11.24.22 \n    Thanksgiving \n     \n     \n     \n     \n  \n  \n    Week 14  11.29.22 \n    • two-sided + • t-tests + •  LRT +  goodness of fit \n    \\makecell[l]{P&S: 9.4-9.6, 10.1  } \n     \n     \n     \n  \n  \n    Week 15  12.6.22 \n    • Bayes tests + • foundational ideas \n    \\makecell[c]{P&S: 9.8-9.9  } \n     \n     \n     \n  \n  \n    Week 14  4.26.22 \n     \n     \n     \n     \n     \n  \n  \n    Thursday  12.15.22  2-5pm \n    Final Exam  \n     \n     \n     \n     \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW2f22.html",
    "href": "handout/MA152_HW2f22.html",
    "title": "Math 152 - Statistical Theory - Homework 2",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW1f22.html",
    "href": "handout/MA152_HW1f22.html",
    "title": "Math 152 - Statistical Theory - Homework 1",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Footnotes\n\n\nadapted from Monica Linden, Brown University↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Statistical Theory",
    "section": "",
    "text": "Footnotes\n\n\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nSort of likely the Green Cab company\n\n↩︎\n\n\\(f( x | \\theta ) = P(X = x | \\theta)\\)\n\n↩︎\n\nintegrate the joint distribution with respect to Y.\n\n↩︎\n\nIntegrate to 1 (\\(dx\\))\n\n↩︎\n\nhas support on [0,1]\n\n↩︎\n\nprior = marginal & posterior = conditional\n\n↩︎\nBoth (b) \\(\\xi(\\theta | \\underline{X}) \\sim\\) Beta (4,12) and (c) \\(\\xi(\\theta | \\underline{X}) \\propto\\) Beta (4,12) are incorrect. (b) because the value to the left of the \\(\\sim\\) must be a random variable. (c) because the value to the right of the \\(\\propto\\) must be a function.↩︎\n\n1/[\\(2^{k/2} \\Gamma(k/2)\\)]\n\n↩︎\n\ngamma\n\n↩︎\n\nit doesn’t integrate to one.\n\n↩︎\n\\(\\mu_1 = \\frac{\\sigma^2 \\mu_0 + n \\nu_0 \\overline{X}}{\\sigma^2 + n \\nu_0^2}\\)↩︎\n\nsome of the above (the Bayes estimator is the posterior mean, it is sensitive to the rest of it.)\n\n↩︎\n\nthe data\n\n↩︎\n\ntheta\n\n↩︎\nwith respect to \\(\\theta\\)↩︎\nL(\\(\\hat{\\theta}\\)) < L(\\(\\theta\\))↩︎\n\nhas desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)\n\n↩︎\n\nis often straightforward to compute (it does not always exist, look at Cauchy. it does not always produce estimates inside the parameter space.)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nthe distribution of the statistic in repeated samples\n\n↩︎\n\nthe cdf, the pdf/pmf, and the mgf\n\n↩︎\n\ngives all theoretical moments of the distribution\n\n↩︎\n(e): (a), (c), (d)↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\na little bit more than 1 (dividing by \\(s\\) instead of \\(\\sigma\\) adds variability to the distribution)\n\n↩︎\n\n50 observations in each bootstrap sample\n\n↩︎\n\n1000\n\n↩︎\n\nthe sample statistic\n\n↩︎\n\nResampling with replacement from the original sample. Although I suppose (c) is also true.\n\n↩︎\n\nThe difference between a sampling distribution mean and the actual parameter.\n\n↩︎\n\n-0.009 Bias is what the statistic is (on average) minus the true value. Recall, we are using the data as a proxy for the population, so the “truth” is the data. So in the bootstrap setting, the average is over the bootstrapped values and the true value is the sample mean.\n\n↩︎\n\n\\(\\sigma^2\\) (the first two are statistics, not parameters, we can’t isolate \\(\\mu\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(\\mu\\) (the first two are statistics, not parameters, we can’t isolate \\(\\sigma^2\\) because it isn’t involved, and \\(\\chi\\) also isn’t a parameter)\n\n↩︎\n\n\\(c_2\\) set to infinity\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nIn many repeated samples, 90% of intervals like this one will contain the true average number of chips.\n\n↩︎\n\nN(0,1/0). Or rather, to get the frequentist result, you need the joint improper priors to have \\(\\mu_0 = \\lambda_0 = \\beta_0 = 0\\) and \\(\\alpha_0 = -1/2\\).\n\n↩︎\n\nThe MGF is usually easiest if g is any kind of linear combination. If not, you might need (b) find the cdf. You’ll need to find the cdf to get the pdf, which you might need to identify the distribution. (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nfind the MGF (note: can’t identify a distribution using only the first two moments, (d))\n\n↩︎\n\nthe parameters from the likelihood\n\n↩︎\n\nthe data and (c) the prior parameters\n\n↩︎\n\n22.5 \\(\\pm\\) z(.975) * 2.334\n\n↩︎\n\nA bootstrap BCa interval (although out of the ones we’ve covered, (b) A bootstrap-t confidence interval is most accurate)\n\n↩︎\n\ncan be done for statistics with unknown sampling distributions\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW3f22.html",
    "href": "handout/MA152_HW3f22.html",
    "title": "Math 152 - Statistical Theory - Homework 3",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/MA152_HW4f22.html",
    "href": "handout/MA152_HW4f22.html",
    "title": "Math 152 - Statistical Theory - Homework 4",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  }
]