<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.256">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Theory – clicker_study</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./images/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="https://m152-stat-theory.netlify.app/" class="navbar-brand navbar-brand-logo">
    <img src="./images/st47s.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="https://m152-stat-theory.netlify.app/">
    <span class="navbar-title">Statistical Theory</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./syllabus.html">
 <span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./notes.html">
 <span class="menu-text">Class Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./clicker_study.html" aria-current="page">
 <span class="menu-text">Clicker Q</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">



<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
<!-- the two formats are html and revealjs -->
<section id="clicker-q" class="level1">
<h1>Clicker Q</h1>
<p>to go with <strong>Probability &amp; Statistics</strong> by DeGroot and Schervish. Math 152 - Statistical Theory.</p>
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
<hr>
<ol type="1">
<li>The Central Limit Theorem (CLT) says:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
<ol type="a">
<li>The sample average (statistic) converges to the true average (parameter)</li>
<li>The sample average (statistic) converges to some point</li>
<li>The distribution of the sample average (statistic) converges to a normal distribution</li>
<li>The distribution of the sample average (statistic) converges to some distribution</li>
<li>I have no idea what the CLT says</li>
</ol></li>
</ol>
<hr>
<ol start="2" type="1">
<li>Which cab company was involved (see example 2.2 in the notes)?<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
<ol type="a">
<li>Very likely the Blue Cab company</li>
<li>Sort of likely the Blue Cab company</li>
<li>Equally likely Blue and Green Cab companies</li>
<li>Sort of likely the Green Cab company</li>
<li>Very likely the Green Cab company</li>
</ol></li>
</ol>
<hr>
<ol start="3" type="1">
<li>Consider a continuous probability density function (pdf) given by <span class="math inline">\(f( x | \theta ).\)</span> Which of the following is FALSE:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>
<ol type="a">
<li><span class="math inline">\(f( x | \theta ) = P(X = x | \theta)\)</span></li>
<li><span class="math inline">\(f( x | \theta )\)</span> provides info for calculating probabilities of X.</li>
<li><span class="math inline">\(P(X = x) = 0\)</span> if X is continuous.</li>
<li><span class="math inline">\(f( x | \theta ) = L(\theta | x)\)</span> is the likelihood function</li>
</ol></li>
</ol>
<hr>
<ol start="4" type="1">
<li>To find a marginal distribution <strong>of X</strong> from a joint distribution <strong>of X &amp; Y</strong> you should (assume everything is continuous),<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>
<ol type="a">
<li>differentiate the joint distribution with respect to X.</li>
<li>differentiate the joint distribution with respect to Y.</li>
<li>integrate the joint distribution with respect to X.</li>
<li>integrate the joint distribution with respect to Y.</li>
<li>I have no idea what a marginal distribution is.</li>
</ol></li>
</ol>
<hr>
<ol start="5" type="1">
<li>A continuous pdf (of a random variable <span class="math inline">\(X\)</span> with parameter <span class="math inline">\(\theta\)</span>) should<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
<ol type="a">
<li>Integrate to a constant (<span class="math inline">\(dx\)</span>)</li>
<li>Integrate to a constant (<span class="math inline">\(d\theta\)</span>)</li>
<li>Integrate to 1 (<span class="math inline">\(dx\)</span>)</li>
<li>Integrate to 1 (<span class="math inline">\(d\theta\)</span>)</li>
<li>not need to integrate to anything special.</li>
</ol></li>
</ol>
<hr>
<ol start="6" type="1">
<li>R / R Studio
<ol type="a">
<li>all good</li>
<li>started, progress is slow and steady</li>
<li>started, very stuck</li>
<li>haven’t started yet</li>
<li>what do you mean by “R”?</li>
</ol></li>
</ol>
<hr>
<ol start="7" type="1">
<li>In terms of the R for the homework…
<ol type="a">
<li>I was able to do the whole thing.</li>
<li>I understood the code part, but I couldn’t get the Markdown file to compile.</li>
<li>I didn’t understand the code at all.</li>
<li>I couldn’t get R or R Studio installed.</li>
<li>I haven’t tried to work on the homework yet.</li>
</ol></li>
</ol>
<hr>
<ol start="8" type="1">
<li>A beta distribution<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>
<ol type="a">
<li>has support on [0,1]</li>
<li>has parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> which represent, respectively, the mean and variance</li>
<li>is discrete</li>
<li>has equal mean and variance</li>
<li>has equal mean and standard deviation</li>
</ol></li>
</ol>
<hr>
<ol start="9" type="1">
<li>What types of distributions are the following?<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>
<ol type="a">
<li>prior = marginal &amp; posterior = joint</li>
<li>prior = joint &amp; posterior = conditional</li>
<li>prior = conditional &amp; posterior = joint</li>
<li>prior = marginal &amp; posterior = conditional</li>
<li>prior = joint &amp; posterior = marginal</li>
</ol></li>
</ol>
<hr>
<ol start="10" type="1">
<li>Which of these are incorrect conclusions?<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
<ol type="a">
<li><span class="math inline">\(\theta | \underline{X} \sim\)</span> Beta (4,12)</li>
<li><span class="math inline">\(\xi(\theta | \underline{X}) \sim\)</span> Beta (4,12)</li>
<li><span class="math inline">\(\xi(\theta | \underline{X}) \propto\)</span> Beta (4,12)</li>
<li><span class="math inline">\(\xi(\theta | \underline{X}) \propto \theta^{4-1} (1-\theta)^{12-1}\)</span></li>
<li><span class="math inline">\(\xi(\theta | \underline{X}) = \frac{1}{B(4,12)} \theta^{4-1}(1-\theta)^{12-1}\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="11" type="1">
<li>What is the integrating constant for the pdf, <span class="math inline">\(h(w)\)</span>?<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>
<ol type="a">
<li><span class="math inline">\(\frac{\Gamma(w+k)}{\Gamma(w)\Gamma(k)}\)</span></li>
<li>1/[<span class="math inline">\(w^k \Gamma(k)\)</span>]</li>
<li>1 / <span class="math inline">\(\sqrt{2\pi k^2}\)</span></li>
<li>1/[<span class="math inline">\(\Gamma(k/2)\)</span>]</li>
<li>1/[<span class="math inline">\(2^{k/2} \Gamma(k/2)\)</span>]</li>
</ol></li>
</ol>
<p><span class="math display">\[h(w) \propto w^{k/2-1}e^{-w/2} \ \ \ \ \ \ \ \ \ w&gt;0\]</span></p>
<hr>
<ol start="12" type="1">
<li>Suppose the data come from an exponential distribution with a parameter whose prior is given by a gamma distribution. The posterior is known to be conjugate, so its distribution must be in what family?<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>
<ol type="a">
<li>exponential</li>
<li>gamma</li>
<li>normal</li>
<li>beta</li>
<li>Poisson</li>
</ol></li>
</ol>
<hr>
<ol start="13" type="1">
<li>A prior is improper if<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>
<ol type="a">
<li>it conveys no real information.</li>
<li>it isn’t conjugate.</li>
<li>it doesn’t integrate to one.</li>
<li>it swears a lot.</li>
<li>it isn’t on your distribution sheet.</li>
</ol></li>
</ol>
<hr>
<ol start="14" type="1">
<li>Given a prior: <span class="math inline">\(\theta \sim N(\mu_0, \nu_0^2)\)</span><br>
And a data likelihood: <span class="math inline">\(X | \theta \sim N(\theta, \sigma^2)\)</span><br>
You collect n data values, what is your best guess of <span class="math inline">\(\theta?\)</span><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>
<ol type="a">
<li><span class="math inline">\(\overline{X}\)</span></li>
<li><span class="math inline">\(\mu_0\)</span></li>
<li><span class="math inline">\(\mu_1 = \frac{\sigma^2 \mu_0 + n \nu_0^2 \overline{X}}{\sigma^2 + n \nu_0^2}\)</span><br>
</li>
<li>median of <span class="math inline">\(N(\mu_1, \nu_1^2 = \frac{\sigma^2 \nu_0^2}{\sigma^2 + n \nu_0^2})\)</span><br>
</li>
<li>47</li>
</ol></li>
</ol>
<hr>
<!--Put in a question with x-bar and the prior mean, what is the posterior mean going to be?  Weighted average… then do it again with a huge n.

---

-->
<ol start="15" type="1">
<li>The Bayes estimator is sensitive to<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>
<ol type="a">
<li>the posterior mean</li>
<li>the prior mean</li>
<li>the sample size</li>
<li>the data values</li>
<li>some of the above</li>
</ol></li>
</ol>
<hr>
<ol start="16" type="1">
<li>The range (output) of the Bayesian MSE includes:<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>
<ol type="a">
<li>theta</li>
<li>the data</li>
</ol></li>
</ol>
<hr>
<ol start="17" type="1">
<li>The range (output) of the frequentist MSE includes:<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>
<ol type="a">
<li>theta</li>
<li>the data</li>
</ol></li>
</ol>
<hr>
<ol start="18" type="1">
<li>To find the maximum likelihood estimator, we take the derivative of the likelihood<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>
<ol type="a">
<li>with respect to <span class="math inline">\(X\)</span></li>
<li>with respect to <span class="math inline">\(\underline{X}\)</span></li>
<li>with respect to <span class="math inline">\(\theta\)</span></li>
<li>with respect to <span class="math inline">\(f\)</span></li>
<li>with respect to <span class="math inline">\(\ln(f)\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="19" type="1">
<li>Consider an MLE, <span class="math inline">\(\hat{\theta},\)</span> and the related log likelihood function <span class="math inline">\(L = \ln(f).\)</span> <span class="math inline">\(\delta(X)\)</span> is another estimate of <span class="math inline">\(\theta\)</span>. Which statement is necessarily false:<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>
<ol type="a">
<li>L(<span class="math inline">\(\delta(X)\)</span>) &lt; L(<span class="math inline">\(\theta\)</span>)</li>
<li>L(<span class="math inline">\(\hat{\theta}\)</span>) &lt; L(<span class="math inline">\(\theta\)</span>)</li>
<li>L(<span class="math inline">\(\theta\)</span>) &lt; L(<span class="math inline">\(\delta(X)\)</span>)</li>
<li>L(<span class="math inline">\(\delta(X)\)</span>) &lt; L(<span class="math inline">\(\hat{\theta}\)</span>)</li>
<li>L(<span class="math inline">\(\theta\)</span>) &lt; L(<span class="math inline">\(\hat{\theta}\)</span>)</li>
</ol></li>
</ol>
<hr>
<ol start="20" type="1">
<li>The MLE is popular because it<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>
<ol type="a">
<li>maximizes <span class="math inline">\(R^2\)</span></li>
<li>minimizes the sum of squared errors</li>
<li>has desirable sampling distribution properties</li>
<li>maximizes both the likelihood and the log likelihood</li>
<li>always exists</li>
</ol></li>
</ol>
<hr>
<ol start="21" type="1">
<li>MOM is popular because it:<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>
<ol type="a">
<li>has desirable sampling properties</li>
<li>is often straightforward to compute</li>
<li>always produces values inside the parameter space (e.g., in [0,1] for prob)</li>
<li>always exists</li>
</ol></li>
</ol>
<hr>
<ol start="22" type="1">
<li>The Central Limit Theorem (CLT) says:<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>
<ol type="a">
<li>The sample average (statistic) converges to the true average (parameter)</li>
<li>The sample average (statistic) converges to some point</li>
<li>The distribution of the sample average (statistic) converges to a normal distribution</li>
<li>The distribution of the sample average (statistic) converges to some distribution</li>
<li>I have no idea what the CLT says</li>
</ol></li>
</ol>
<hr>
<ol start="23" type="1">
<li>A sampling distribution is<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>
<ol type="a">
<li>the true distribution of the data</li>
<li>the estimated distribution of the data</li>
<li>the distribution of the population</li>
<li>the distribution of the statistic in repeated samples</li>
<li>the distribution of the statistic from your one sample of data</li>
</ol></li>
</ol>
<hr>
<ol start="24" type="1">
<li>The distribution of a random variable can be uniquely determined by<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>
<ol type="a">
<li>the cdf: F(x)</li>
<li>the pdf (pmf): f(x)</li>
<li>the moment generating function (mgf), if it exists: <span class="math inline">\(\Psi(t) = E[e^{tX}]\)</span></li>
<li>the mean and variance of the distribution</li>
<li>more than one of the above (which ones??)</li>
</ol></li>
</ol>
<hr>
<ol start="25" type="1">
<li>A moment generating function<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>
<ol type="a">
<li>gives the probability of the RV at any value of X</li>
<li>gives all theoretical moments of the distribution</li>
<li>gives all sample moments of the data</li>
<li>gives the cumulative probability of the RV at any value of X</li>
</ol></li>
</ol>
<hr>
<ol start="26" type="1">
<li>The sampling distribution is important because<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>
<ol type="a">
<li>it describes the behavior (distribution) of the statistic</li>
<li>it describes the behavior (distribution) of the data</li>
<li>it gives us the ability to measure the likelihood of the statistic or more extreme under particular settings (i.e.&nbsp;null)</li>
<li>it gives us the ability to make inferences about the population parameter</li>
<li>more than one of the above (which ones??)</li>
</ol></li>
</ol>
<hr>
<ol start="27" type="1">
<li>The following result: <span class="math inline">\(\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}\)</span> allows us to isolate and conduct inference on what parameter?<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>
<ol type="a">
<li><span class="math inline">\(\overline{X}\)</span></li>
<li><span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(\chi\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="28" type="1">
<li>The following result: <span class="math inline">\(\frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}\)</span> allows us to isolate and conduct inference on what parameter?<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>
<ol type="a">
<li><span class="math inline">\(\overline{X}\)</span></li>
<li><span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(\chi\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="29" type="1">
<li>What would you expect the standard deviation of the t statistic to be?<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>
<ol type="a">
<li>a little bit less than 1</li>
<li>1</li>
<li>a little bit more than 1</li>
<li>unable to tell because it depends on the sample size and the variability of the data</li>
</ol></li>
</ol>
<hr>
<ol start="30" type="1">
<li>You have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. What is the sample size of each bootstrap sample?<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>
<ol type="a">
<li>50</li>
<li>1000</li>
</ol></li>
</ol>
<hr>
<ol start="31" type="1">
<li>You have a sample of size n = 50. You sample with replacement 1000 times to get 1000 bootstrap samples. How many bootstrap statistics will you have?<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>
<ol type="a">
<li>50</li>
<li>1000</li>
</ol></li>
</ol>
<hr>
<ol start="32" type="1">
<li>The bootstrap distribution of <span class="math inline">\(\hat{\theta}\)</span> is centered around the<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>
<ol type="a">
<li>population parameter</li>
<li>sample statistic</li>
<li>bootstrap statistic</li>
<li>bootstrap parameter</li>
</ol></li>
</ol>
<hr>
<ol start="33" type="1">
<li>The bootstrap theory relies on<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>
<ol type="a">
<li>Resampling with replacement from the original sample.</li>
<li>Resampling from the original sample, leaving one observation out each time (e.g., cross validation)</li>
<li>Estimating the population using the sample.</li>
<li>Permuting the data values within the sample.</li>
</ol></li>
</ol>
<hr>
<ol start="34" type="1">
<li>Bias of a statistic refers to<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>
<ol type="a">
<li>The difference between a statistic and the actual parameter</li>
<li>Whether or not questions were worded fairly.</li>
<li>The difference between a sampling distribution mean and the actual parameter.</li>
</ol></li>
</ol>
<hr>
<ol start="35" type="1">
<li>The mean of a sample is 22.5. The mean of 1000 bootstrapped samples is 22.491. The bias of the bootstrap mean is<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>
<ol type="a">
<li>-0.009</li>
<li>-0.0045</li>
<li>-0.09</li>
<li>0.009</li>
<li>0.09</li>
</ol></li>
</ol>
<hr>
<ol start="36" type="1">
<li>The following result: <span class="math inline">\(\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}\)</span> allows us to isolate and conduct inference on what parameter?<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>
<ol type="a">
<li><span class="math inline">\(\overline{X}\)</span></li>
<li><span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(\chi\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="37" type="1">
<li>The following result: <span class="math inline">\(\frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}\)</span><br>
allows us to isolate and conduct inference on what parameter?<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>
<ol type="a">
<li><span class="math inline">\(\overline{X}\)</span></li>
<li><span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(\chi\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="38" type="1">
<li>Consider an asymmetric confidence interval for <span class="math inline">\(\sigma\)</span> which is derived using:<br>
<span class="math inline">\(P(c_1 \leq \frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{\sigma^2} \leq c_2) = 0.95\)</span><br>
The resulting 95% interval with the <strong>shortest</strong> width has:<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>
<ol type="a">
<li><span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> as the .025 &amp; .975 quantiles</li>
<li><span class="math inline">\(c_1\)</span> set to zero</li>
<li><span class="math inline">\(c_2\)</span> set to infinity</li>
<li><span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> as different quantiles than (a) but that contain .95 probability.</li>
<li>Find <span class="math inline">\(c_1\)</span> and let <span class="math inline">\(c_2 = -c_1\)</span></li>
</ol></li>
</ol>
<hr>
<!--

not a good problem because we don't know the skew of the distribution of sigma2

39. Consider an asymmetric posterior distribution which gives an interval using:  
$P((c_1 \leq \sigma^2 \leq c_2) | \underline{X})= 0.95$  
The resulting 95% interval with the **shortest** width has:[^39]
    (a) $c_1$ and $c_2$ as the .025 & .975 quantiles
    (b) $c_1$ set to zero
    (c) $c_2$ set to infinity
    (d) $c_1$ and $c_2$ as different quantiles than (a) but that contain .95 probability.
    (e) Find $c_1$ and let $c_2 = -c_1$

[^39]: (a) $c_1$ and $c_2$ as the .025 & .975 quantiles


---
-->
<ol start="39" type="1">
<li>A 90% CI for the average number of chocolate chips in a Chips Ahoy cookie is: [3.7 chips, 17.2 chips]<br>
What is the correct interpretation?<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>
<ol type="a">
<li>There is a 0.9 prob that the true average number of chips is between 3.7 &amp; 17.2.</li>
<li>90% of cookies have between 3.7 &amp; 17.2 chips.</li>
<li>We are 90% confident that in our sample, the sample average number of chips is between 3.7 and 17.2.</li>
<li>In many repeated samples, 90% of sample averages will be between 3.7 and 17.2.</li>
<li>In many repeated samples, 90% of intervals like this one will contain the true average number of chips.</li>
</ol></li>
</ol>
<hr>
<ol start="40" type="1">
<li>A 90% CI for the average number of chocolate chips in a Chips Ahoy cookie: [3.9 chips, <span class="math inline">\(\infty\)</span>)<br>
What is the correct interpretation?<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>
<ol type="a">
<li>There is a 0.9 prob that the true average number of chips is bigger than 3.9</li>
<li>90% of cookies have more than 3.9 chips</li>
<li>We are 90% confident that in our sample, the sample average number of chips is bigger than 3.9.</li>
<li>In many repeated samples, 90% of sample averages will be bigger than 3.9</li>
<li>In many repeated samples, 90% of intervals like this one will contain the true average number of chips.</li>
</ol></li>
</ol>
<hr>
<ol start="41" type="1">
<li>Consider a Bayesian posterior interval for <span class="math inline">\(\mu\)</span> of the form: <span class="math inline">\(\overline{X} \pm t^*_{n-1} s / \sqrt{n}\)</span><br>
What was the prior on <span class="math inline">\(\mu\)</span>?<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>
<ol type="a">
<li>N(0,0)</li>
<li>N(<span class="math inline">\(\overline{X}\)</span>,0)</li>
<li>N(0, 1/0)</li>
<li>N(<span class="math inline">\(\overline{X}\)</span>,1/0)</li>
<li>N(1/0, 0)</li>
</ol></li>
</ol>
<hr>
<p>Some review questions:</p>
<ol start="42" type="1">
<li>If we need to find the distribution of a function of one variable (g(X) = X), the easiest route is probably:<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>
<ol type="a">
<li>find the pdf</li>
<li>find the cdf</li>
<li>find the MGF</li>
<li>find the expected value and variance</li>
</ol></li>
</ol>
<hr>
<ol start="43" type="1">
<li>If we need to find the distribution of a sum of random variables, the easiest route is probably:<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>
<ol type="a">
<li>find the pdf</li>
<li>find the cdf</li>
<li>find the MGF</li>
<li>expected value and variance</li>
</ol></li>
</ol>
<hr>
<ol start="44" type="1">
<li>FREQUENTIST: consider the sampling distribution of <span class="math inline">\(\hat{\theta}.\)</span> The parameters in the sampling distribution are given by:<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>
<ol type="a">
<li>the data</li>
<li>the parameters from the likelihood</li>
<li>the prior parameters</li>
<li>the statistic</li>
<li><span class="math inline">\(\theta\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="45" type="1">
<li>BAYESIAN: consider the posterior distribution of <span class="math inline">\(\theta | \underline{X}.\)</span> The parameters in the posterior distribution are a function of:<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a>
<ol type="a">
<li>the data</li>
<li>the parameters from the likelihood</li>
<li>the prior parameters</li>
<li>the statistic</li>
<li><span class="math inline">\(\theta\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="46" type="1">
<li>A sample of size 8 had a mean of 22.5. It was bootstrapped 1000 times and the mean of the bootstrap distribution was 22.491. The standard deviation of the bootstrap was 2.334. The 95% BS SE confidence interval for the population mean is<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>
<ol type="a">
<li>22.491 <span class="math inline">\(\pm\)</span> z(.975) * 2.334</li>
<li>22.491 <span class="math inline">\(\pm\)</span> z(.95) * 2.334</li>
<li>22.5 <span class="math inline">\(\pm\)</span> z(.975) * 2.334</li>
<li>22.5 <span class="math inline">\(\pm\)</span> z(.95) * 2.334</li>
<li>22.5 <span class="math inline">\(\pm\)</span> z(.975) * 2.334 / <span class="math inline">\(\sqrt{8}\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="47" type="1">
<li>Which is most accurate?<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>
<ol type="a">
<li>A BS SE confidence interval</li>
<li>A bootstrap-t confidence interval</li>
<li>A bootstrap percentile interval</li>
<li>A bootstrap BCa interval</li>
</ol></li>
</ol>
<hr>
<ol start="48" type="1">
<li>What is the primary reason to bootstrap a CI (instead of creating a CI from calculus)?<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a>
<ol type="a">
<li>larger coverage probabilities</li>
<li>narrower intervals</li>
<li>more resistant to outliers</li>
<li>can be done for statistics with unknown sampling distributions</li>
</ol></li>
</ol>
<hr>
<ol start="49" type="1">
<li>What does the Fisher Information tell us?<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a>
<ol type="a">
<li>the variability of the MLE from sample to sample.</li>
<li>the bias of the MLE from sample to sample.</li>
<li>the variability of the data from sample to sample.</li>
<li>the bias of the data from sample to sample.</li>
</ol></li>
</ol>
<hr>
<ol start="50" type="1">
<li>Why do we care about the variability of the MLE?<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a>
<ol type="a">
<li>determines whether MOM or MLE is better.</li>
<li>determines whether Bayes’ estimator or MLE is better.</li>
<li>determines how precise the estimator is.</li>
<li>allows us to do inference (about the population value).</li>
</ol></li>
</ol>
<hr>
<ol start="51" type="1">
<li>Why do we care about the sampling distribution of the MLE?<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>
<ol type="a">
<li>determines whether MOM or MLE is better.</li>
<li>determines whether Bayes’ estimator or MLE is better.</li>
<li>determines how precise the estimator is.</li>
<li>allows us to do inference (about the population value).</li>
</ol></li>
</ol>
<hr>
<ol start="52" type="1">
<li>Consider an estimator, <span class="math inline">\(\hat{\theta}\)</span>, such that <span class="math inline">\(E[\hat{\theta}] = m(\theta)\)</span>.<br>
<span class="math inline">\(\hat{\theta}\)</span> is unbiased for <span class="math inline">\(\theta\)</span> if:<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a>
<ol type="a">
<li><span class="math inline">\(m(\theta)\)</span> is a function of <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(m(\theta)\)</span> is NOT a function of <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(m(\theta)= \theta\)</span>.</li>
<li><span class="math inline">\(m(\theta)= 0\)</span>.</li>
<li><span class="math inline">\(m(\theta)\)</span> is the expected value of <span class="math inline">\(\hat{\theta}\)</span>.</li>
</ol></li>
</ol>
<hr>
<ol start="53" type="1">
<li>If <span class="math inline">\(\hat{\theta}\)</span> is unbiased, <span class="math inline">\(m'(\theta)\)</span> is<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a>
<ol type="a">
<li>zero</li>
<li>one</li>
<li><span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(\theta^2\)</span></li>
<li>some other function of <span class="math inline">\(\theta\)</span>, depending on <span class="math inline">\(m(\theta)\)</span></li>
</ol></li>
</ol>
<hr>
<ol start="54" type="1">
<li>The MLE is<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>
<ol type="a">
<li>consistent</li>
<li>efficient</li>
<li>asymptotically normally distributed</li>
<li>all of the above</li>
</ol></li>
</ol>
<hr>
<ol start="55" type="1">
<li>Why don’t we set up our test as: always reject <span class="math inline">\(H_0?\)</span><a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a>
<ol type="a">
<li>type I error too high</li>
<li>type II error too high</li>
<li>level of sig too high</li>
<li>power too high</li>
</ol></li>
</ol>
<hr>
<ol start="56" type="1">
<li>Why do we care about the distribution of the test statistic?<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a>
<ol type="a">
<li>Better estimator</li>
<li>To find the rejection region / critical region</li>
<li>To minimize the power</li>
<li>Because we love the Central Limit Theorem</li>
</ol></li>
</ol>
<hr>
<ol start="57" type="1">
<li>Given a statistic T = r(X), how do we find a (good) test?<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a>
<ol type="a">
<li>Maximize power when <span class="math inline">\(H_1\)</span> is true</li>
<li>Minimize type I error</li>
<li>Control type I error</li>
<li>Minimize type II error</li>
<li>Control type II error</li>
</ol></li>
</ol>
<hr>
<ol start="58" type="1">
<li>We can find the probability of type II error (at a given <span class="math inline">\(\theta \in \Omega_1)\)</span> as<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>
<ol type="a">
<li>a value of the power curve (at <span class="math inline">\(\theta)\)</span></li>
<li>1 – P(type I error at <span class="math inline">\(\theta)\)</span></li>
<li><span class="math inline">\(\pi(\theta | \delta)\)</span></li>
<li>1- <span class="math inline">\(\pi(\theta | \delta)\)</span></li>
<li>we can’t ever find the probability of a type II error</li>
</ol></li>
</ol>
<hr>
<ol start="59" type="1">
<li>Why don’t we use the power function to <strong>also</strong> control the type II error?<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a> (We want the power to be big in <span class="math inline">\(\Omega_1\)</span>, so we’d control it by keeping the power from getting too small.)
<ol type="a">
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> does not exist</li>
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> =0</li>
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> = always really big</li>
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> =1</li>
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> = always really small</li>
</ol></li>
</ol>
<hr>
<ol start="60" type="1">
<li>With two simple hypotheses, hypothesis testing simplifies because we can now control (i.e., compute):<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a>
<ol type="a">
<li>the size of the test.</li>
<li>the power of the test.</li>
<li>the probability of type I error.</li>
<li>the probability of type II error.</li>
<li>a rejection region.</li>
</ol></li>
</ol>
<hr>
<ol start="61" type="1">
<li>The likelihood ratio is super awesome because<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a>
<ol type="a">
<li>it provides the test statistic</li>
<li>it provides the critical region</li>
<li>it provides the type I error</li>
<li>it provides the type II error</li>
<li>it provides the power</li>
</ol></li>
</ol>
<hr>
<ol start="62" type="1">
<li>A uniformly most powerful (UMP) test<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a>
<ol type="a">
<li>has the highest possible power in <span class="math inline">\(\Omega_1\)</span>.</li>
<li>has the lowest possible power in <span class="math inline">\(\Omega_1\)</span>.</li>
<li>has the same power over all <span class="math inline">\(\theta \in \Omega_1\)</span>.</li>
<li>has the highest possible power in <span class="math inline">\(\Omega_1\)</span> subject to controlling <span class="math inline">\(\alpha(\delta).\)</span></li>
<li>is a test we try to avoid.</li>
</ol></li>
</ol>
<hr>
<ol start="63" type="1">
<li>A monotone likelihood ratio statistic is awesome because<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a>
<ol type="a">
<li>it is the MLE</li>
<li>it is easy to compute</li>
<li>its distribution is known</li>
<li>it is unbiased</li>
<li>it is monotonic with respect to the likelihood ratio</li>
</ol></li>
</ol>
<hr>
<ol start="64" type="1">
<li>Likelihood Ratio Test<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a>
<ol type="a">
<li>gives a statistic for comparing likelihoods</li>
<li>is always UMP</li>
<li>works only with some types of hypotheses</li>
<li>works only with hypotheses about one parameter</li>
<li>gives the distribution of the test statistic</li>
</ol></li>
</ol>
<hr>
<ol start="65" type="1">
<li>Increasing sample size<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a>
<ol type="a">
<li>Increases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
<li>Decreases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol></li>
</ol>
<hr>
<ol start="66" type="1">
<li>Making significance level more stringent (<span class="math inline">\(\alpha_0\)</span> smaller)<a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a>
<ol type="a">
<li>Increases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
<li>Decreases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol></li>
</ol>
<hr>
<ol start="67" type="1">
<li>A more extreme alternative is true<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a>
<ol type="a">
<li>Increases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
<li>Decreases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol></li>
</ol>
<hr>
<ol start="68" type="1">
<li>Given the situation where <span class="math inline">\(H_1: \mu_1 - \mu_2 \ne 0\)</span> is TRUE. Consider 100 CIs (for <span class="math inline">\(\mu_1 - \mu_2\)</span>), the power of the test can be approximated by:<a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a>
<ol type="a">
<li>The proportion that contain the true mean.</li>
<li>The proportion that do not contain the true mean.</li>
<li>The proportion that contain zero.</li>
<li>The proportion that do not contain zero.</li>
</ol></li>
</ol>
<hr>
<ol start="69" type="1">
<li>It is hard to find the power associated with the t-test because:<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a>
<ol type="a">
<li>the non-central t-distribution is tricky.</li>
<li>two-sided power is difficult to find.</li>
<li>we don’t know the variance.</li>
<li>the t-distribution isn’t integrable.</li>
</ol></li>
</ol>
<hr>
<ol start="70" type="1">
<li>Consider the likelihood ratio statistic: <span class="math display">\[\Lambda(x) = \frac{\sup_{\Omega_1} f(\underline{x} | \theta)}{\sup_{\Omega_0} f(\underline{x} | \theta)}\]</span> Why do we assume that the MLE maximizes the numerator?<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a>
<ol type="a">
<li>The MLE is always in the alternative space.</li>
<li>The MLE is always in the null space.</li>
<li>If the MLE is in the alternative space, we won’t reject <span class="math inline">\(H_0\)</span>.</li>
<li>If the MLE is in the null space, we won’t reject <span class="math inline">\(H_0\)</span>.</li>
<li>If the MLE is in the alternative space, we will reject <span class="math inline">\(H_0\)</span>.</li>
</ol></li>
</ol>
<hr>
<ol start="71" type="1">
<li>Consider the likelihood ratio statistic:<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a> <span class="math display">\[\Lambda(x) = \frac{\sup_{\Omega_1} f(\underline{x} | \theta)}{\sup_{\Omega_0} f(\underline{x} | \theta)}\]</span>
<ol type="a">
<li><span class="math inline">\(\Lambda(x) \geq 1\)</span></li>
<li><span class="math inline">\(\Lambda(x) \leq 1\)</span></li>
<li><span class="math inline">\(\Lambda(x) \geq 0\)</span></li>
<li><span class="math inline">\(\Lambda(x) \leq 0\)</span></li>
<li>bounds on <span class="math inline">\(\Lambda(x)\)</span> depend on hypotheses</li>
</ol></li>
</ol>
<hr>
<ol start="72" type="1">
<li>When using the chi-square goodness of fit test, the smaller the value of the chi-square test statistic, the more likely we are to reject the null hypothesis.<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a>
<ol type="a">
<li>True</li>
<li>False</li>
</ol></li>
</ol>
<hr>
<ol start="73" type="1">
<li>A chi-square test is<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a>
<ol type="a">
<li>one-sided, and we only consider the upper end of the sampling distribution</li>
<li>one-sided, and we consider both ends of the sampling distribution</li>
<li>two-sided, and we only consider the upper end of the sampling distribution</li>
<li>two-sided, and we consider both ends of the sampling distribution</li>
</ol></li>
</ol>
<hr>
<ol start="74" type="1">
<li>To test whether the data are Poisson, why can’t we use the Poisson likelihood instead of the multinomial?<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>
<ol type="a">
<li>Likelihood under <span class="math inline">\(H_0\)</span> is too hard to write down</li>
<li>Likelihood under <span class="math inline">\(H_1\)</span> is too hard to write down</li>
<li>Don’t know the distribution of the corresponding test statistic</li>
<li>Don’t have any data to use</li>
</ol></li>
</ol>
<hr>
<ol start="75" type="1">
<li>The <span class="math inline">\(\chi^2\)</span> test statistic is being used to test whether the assumption of normality is reasonable for a given population distribution. The sample consists of 5000 observations and is divided into 6 categories (intervals). What are the degrees of freedom associated with the test statistic?<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>
<ol type="a">
<li>4999</li>
<li>6</li>
<li>5</li>
<li>4</li>
<li>3</li>
</ol></li>
</ol>
<hr>
<ol start="76" type="1">
<li>For a chi-square test for independence, the null hypothesis states that the two variables<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a>
<ol type="a">
<li>are mutually exclusive.</li>
<li>form a contingency table with r rows and c columns.</li>
<li>have (r –1) and (c –1) degrees of freedom where r and c are the number of rows and columns, respectively.</li>
<li>are statistically independent.</li>
<li>are normally distributed.</li>
</ol></li>
</ol>
<hr>
<ol start="77" type="1">
<li>You read a paper where a chi-square test produces a p-value of 0.999 (not 0.001). You think:<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a>
<ol type="a">
<li><span class="math inline">\(H_0\)</span> is definitely true</li>
<li><span class="math inline">\(H_0\)</span> is definitely not true</li>
<li>The authors’ hypothesis is in the wrong direction.</li>
<li>Maybe they falsified their data?</li>
</ol></li>
</ol>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><ol start="3" type="a">
<li>The distribution of the sample average (statistic) converges to a normal distribution</li>
</ol>
<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn2"><ol start="4" type="a">
<li>Sort of likely the Green Cab company</li>
</ol>
<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn3"><ol type="a">
<li><span class="math inline">\(f( x | \theta ) = P(X = x | \theta)\)</span></li>
</ol>
<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn4"><ol start="4" type="a">
<li>integrate the joint distribution with respect to Y.</li>
</ol>
<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn5"><ol start="3" type="a">
<li>Integrate to 1 (<span class="math inline">\(dx\)</span>)</li>
</ol>
<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn6"><ol type="a">
<li>has support on [0,1]</li>
</ol>
<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn7"><ol start="4" type="a">
<li>prior = marginal &amp; posterior = conditional</li>
</ol>
<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn8"><p>Both (b) <span class="math inline">\(\xi(\theta | \underline{X}) \sim\)</span> Beta (4,12) and (c) <span class="math inline">\(\xi(\theta | \underline{X}) \propto\)</span> Beta (4,12) are incorrect. (b) because the value to the left of the <span class="math inline">\(\sim\)</span> must be a random variable. (c) because the value to the right of the <span class="math inline">\(\propto\)</span> must be a function.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><ol start="5" type="a">
<li>1/[<span class="math inline">\(2^{k/2} \Gamma(k/2)\)</span>]</li>
</ol>
<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn10"><ol start="2" type="a">
<li>gamma</li>
</ol>
<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn11"><ol start="3" type="a">
<li>it doesn’t integrate to one.</li>
</ol>
<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn12"><p><span class="math inline">\(\mu_1 = \frac{\sigma^2 \mu_0 + n \nu_0 \overline{X}}{\sigma^2 + n \nu_0^2}\)</span><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><ol start="5" type="a">
<li>some of the above (the Bayes estimator <strong>is</strong> the posterior mean, it is sensitive to the rest of it.)</li>
</ol>
<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn14"><ol start="2" type="a">
<li>the data</li>
</ol>
<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn15"><ol type="a">
<li>theta</li>
</ol>
<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn16"><p>with respect to <span class="math inline">\(\theta\)</span><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>L(<span class="math inline">\(\hat{\theta}\)</span>) &lt; L(<span class="math inline">\(\theta\)</span>)<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><ol start="3" type="a">
<li>has desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)</li>
</ol>
<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn19"><ol start="2" type="a">
<li>is often straightforward to compute (it does not always exist, look at Cauchy. it does not always produce estimates inside the parameter space.)</li>
</ol>
<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn20"><ol start="3" type="a">
<li>The distribution of the sample average (statistic) converges to a normal distribution</li>
</ol>
<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn21"><ol start="4" type="a">
<li>the distribution of the statistic in repeated samples</li>
</ol>
<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn22"><ol start="5" type="a">
<li>the cdf, the pdf/pmf, and the mgf</li>
</ol>
<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn23"><ol start="2" type="a">
<li>gives all theoretical moments of the distribution</li>
</ol>
<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn24"><p>(e): (a), (c), (d)<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><ol start="4" type="a">
<li><span class="math inline">\(\sigma^2\)</span> (the first two are statistics, not parameters, we can’t isolate <span class="math inline">\(\mu\)</span> because it isn’t involved, and <span class="math inline">\(\chi\)</span> also isn’t a parameter)</li>
</ol>
<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn26"><ol start="3" type="a">
<li><span class="math inline">\(\mu\)</span> (the first two are statistics, not parameters, we can’t isolate <span class="math inline">\(\sigma^2\)</span> because it isn’t involved, and <span class="math inline">\(\chi\)</span> also isn’t a parameter)</li>
</ol>
<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn27"><ol start="3" type="a">
<li>a little bit more than 1 (dividing by <span class="math inline">\(s\)</span> instead of <span class="math inline">\(\sigma\)</span> adds variability to the distribution)</li>
</ol>
<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn28"><ol type="a">
<li>50 observations in each bootstrap sample</li>
</ol>
<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn29"><ol start="2" type="a">
<li>1000</li>
</ol>
<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn30"><ol start="2" type="a">
<li>the sample statistic</li>
</ol>
<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn31"><ol type="a">
<li>Resampling with replacement from the original sample. Although I suppose (c) is also true.</li>
</ol>
<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn32"><ol start="3" type="a">
<li>The difference between a sampling distribution mean and the actual parameter.</li>
</ol>
<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn33"><ol type="a">
<li>-0.009 Bias is what the statistic is (on average) minus the true value. Recall, we are using the data as a proxy for the population, so the “truth” is the data. So in the bootstrap setting, the average is over the bootstrapped values and the true value is the sample mean.</li>
</ol>
<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn34"><ol start="4" type="a">
<li><span class="math inline">\(\sigma^2\)</span> (the first two are statistics, not parameters, we can’t isolate <span class="math inline">\(\mu\)</span> because it isn’t involved, and <span class="math inline">\(\chi\)</span> also isn’t a parameter)</li>
</ol>
<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn35"><ol start="3" type="a">
<li><span class="math inline">\(\mu\)</span> (the first two are statistics, not parameters, we can’t isolate <span class="math inline">\(\sigma^2\)</span> because it isn’t involved, and <span class="math inline">\(\chi\)</span> also isn’t a parameter)</li>
</ol>
<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn36"><ol start="3" type="a">
<li><span class="math inline">\(c_2\)</span> set to infinity</li>
</ol>
<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn37"><ol start="5" type="a">
<li>In many repeated samples, 90% of intervals like this one will contain the true average number of chips.</li>
</ol>
<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn38"><ol start="5" type="a">
<li>In many repeated samples, 90% of intervals like this one will contain the true average number of chips.</li>
</ol>
<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn39"><ol type="a">
<li>N(0,1/0). Or rather, to get the frequentist result, you need the joint improper priors to have <span class="math inline">\(\mu_0 = \lambda_0 = \beta_0 = 0\)</span> and <span class="math inline">\(\alpha_0 = -1/2\)</span>.</li>
</ol>
<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn40"><ol start="3" type="a">
<li>The MGF is usually easiest if g is any kind of linear combination. If not, you might need (b) find the cdf. You’ll need to find the cdf to get the pdf, which you might need to identify the distribution. (note: can’t identify a distribution using only the first two moments, (d))</li>
</ol>
<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn41"><ol start="3" type="a">
<li>find the MGF (note: can’t identify a distribution using only the first two moments, (d))</li>
</ol>
<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn42"><ol start="2" type="a">
<li>the parameters from the likelihood</li>
</ol>
<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn43"><ol type="a">
<li>the data and (c) the prior parameters</li>
</ol>
<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn44"><ol start="3" type="a">
<li>22.5 <span class="math inline">\(\pm\)</span> z(.975) * 2.334</li>
</ol>
<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn45"><ol start="4" type="a">
<li>A bootstrap BCa interval (although out of the ones we’ve covered, (b) A bootstrap-t confidence interval is most accurate)</li>
</ol>
<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn46"><ol start="4" type="a">
<li>can be done for statistics with unknown sampling distributions</li>
</ol>
<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn47"><ol type="a">
<li>the variability of the MLE from sample to sample.</li>
</ol>
<a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn48"><ol start="3" type="a">
<li>determines how precise the estimator is.</li>
</ol>
<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn49"><ol start="4" type="a">
<li>allows us to do inference (about the population value).</li>
</ol>
<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn50"><ol start="3" type="a">
<li><span class="math inline">\(m(\theta)= \theta\)</span>.</li>
</ol>
<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn51"><ol start="2" type="a">
<li>one</li>
</ol>
<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn52"><ol start="4" type="a">
<li>all of the above</li>
</ol>
<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn53"><ol type="a">
<li>type I error too high</li>
</ol>
<a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn54"><ol start="2" type="a">
<li>To find the rejection region / critical region</li>
</ol>
<a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn55"><ol start="3" type="a">
<li>Control type I error</li>
</ol>
<a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn56"><ol start="4" type="a">
<li>1- <span class="math inline">\(\pi(\theta | \delta)\)</span></li>
</ol>
<a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn57"><ol start="5" type="a">
<li><span class="math inline">\(\inf_{\theta \in \Omega_1} \pi(\theta | \delta)\)</span> = always really small</li>
</ol>
<a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn58"><ol start="2" type="a">
<li>the power of the test. or (d) the probability of type II error. (they are functions of one another)</li>
</ol>
<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn59"><ol type="a">
<li>it provides the test statistic</li>
</ol>
<a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn60"><ol start="4" type="a">
<li>has the highest possible power in <span class="math inline">\(\Omega_1\)</span> subject to controlling <span class="math inline">\(\alpha(\delta).\)</span></li>
</ol>
<a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn61"><ol start="5" type="a">
<li>it is monotonic with respect to the likelihood ratio</li>
</ol>
<a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn62"><ol start="5" type="a">
<li>gives the distribution of the test statistic</li>
</ol>
<a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn63"><ol type="a">
<li>Increases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol>
<a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn64"><ol start="2" type="a">
<li>Decreases power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol>
<a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn65"><ol type="a">
<li>Increases your power (over <span class="math inline">\(\Omega_1\)</span>)</li>
</ol>
<a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn66"><ol start="4" type="a">
<li>The proportion that do not contain zero.</li>
</ol>
<a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn67"><ol type="a">
<li>the non-central t-distribution is tricky.</li>
</ol>
<a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn68"><ol start="4" type="a">
<li>If the MLE is in the null space, we won’t reject <span class="math inline">\(H_0\)</span>.</li>
</ol>
<a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn69"><p><span class="math inline">\(\Lambda(x) \geq 1\)</span><a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn70"><ol start="2" type="a">
<li>False</li>
</ol>
<a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn71"><ol start="3" type="a">
<li>two-sided, and we only consider the upper end of the sampling distribution</li>
</ol>
<a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn72"><ol start="2" type="a">
<li>Likelihood under H1 is too hard to write down (what likelihood would we use for the situation of “not Poisson”?)</li>
</ol>
<a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn73"><ol start="3" type="a">
<li>5</li>
</ol>
<a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn74"><ol start="4" type="a">
<li>are statistically independent.</li>
</ol>
<a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn75"><ol start="4" type="a">
<li>Maybe they falsified their data?</li>
</ol>
<a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><a href="https://hardin47.netlify.app/research/"><i class="fa-solid fa-book" aria-hidden="true"></i></a> <a href="https://hardin47.netlify.app/resume.pdf" target="_blank"><i class="fa-solid fa-bookmark"></i></a> <a href="mailto:jo.hardin@pomona.edu" target="_blank"><i class="fa-solid fa-at"></i></a> <a href="https://scholar.google.com/citations?user=c5Y77poAAAAJ&amp;hl=en" target="_blank"><i class="fa-brands fa-google-scholar"></i></a> <a href="https://www.researchgate.net/scientific-contributions/Johanna-Hardin-9764530" target="_blank"><i class="fa-brands fa-researchgate"></i></a><br>
<a href="https://mobile.twitter.com/jo_hardin47" target="_blank"><i class="fa-brands fa-twitter"></i></a> <a href="https://github.com/hardin47" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jo-hardin-b91a4b55" target="_blank"><i class="fa-brands fa-linkedin-in"></i></a></div>   
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item">
 <br> © Jo Hardin, 2022 -- <a href="mailto: jo.hardin@pomona.edu">jo.hardin@pomona.edu</a> -- <a href="https://hardin47.netlify.app/">https://hardin47.netlify.app/</a> <br> <br>
Math 152 -- Statistical Theory -- Pomona College, Claremont, CA <br>
  </li>  
</ul>
      </div>
  </div>
</footer>



</body></html>