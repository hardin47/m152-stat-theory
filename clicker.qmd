---
---
<!-- the two formats are html and revealjs -->

# Clicker Q

to go with **Probability & Statistics** by DeGroot and Schervish.  Math 152 - Statistical Theory.

```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```


---

1. The Central Limit Theorem (CLT) says:[^1]
   a. The sample average (statistic) converges to the true   average (parameter)
   b. The sample average (statistic) converges to some point
   c. The distribution of the sample average (statistic) converges to a normal distribution
   d. The distribution of the sample average (statistic) converges to some distribution
   e. I have no idea what the CLT says
   

[^1]:  c. The distribution of the sample average (statistic) converges to a normal distribution

---

2. Which cab company was involved (see example 2.2 in the notes)?[^2]
    (a) Very likely the Blue Cab company
    (b) Sort of likely the Blue Cab company
    (c) Equally likely Blue and Green Cab companies
    (d) Sort of likely the Green Cab company
    (e) Very likely the Green Cab company

[^2]: (d) Sort of likely the Green Cab company
---

3.  Consider a continuous probability density function (pdf) given by $f( x | \theta ).$ 
Which of the following is FALSE:[^3]
    (a)	$f( x | \theta )  = P(X = x | \theta)$
    (b)	$f( x | \theta )$ provides info for calculating probabilities of X.
    (c)	$P(X = x) = 0$ if X is continuous.
    (d)	$f( x | \theta )  = L(\theta | x)$ is the likelihood function

[^3]: (a)	$f( x | \theta )  = P(X = x | \theta)$

---

4. To find a marginal distribution **of X** from a joint distribution **of X & Y** you should (assume everything is continuous),[^4]
    (a) differentiate the joint distribution with respect to X.
    (b) differentiate the joint distribution with respect to Y.
    (c) integrate the joint distribution with respect to X.
    (d) integrate the joint distribution with respect to Y.
    (e) I have no idea what a marginal distribution is.
    
[^4]: (d) integrate the joint distribution with respect to Y.

---

5. A continuous pdf (of a random variable $X$ with parameter $\theta$) should[^5]
   (a) Integrate to a constant ($dx$)
   (b) Integrate to a constant ($d\theta$)
   (c) Integrate to 1 ($dx$)
   (d) Integrate to 1 ($d\theta$)
   (e) not need to integrate to anything special.

[^5]: (c) Integrate to 1 ($dx$)

---

6. R / R Studio
   (a) all good
   (b) started, progress is slow and steady
   (c) started, very stuck
   (d) haven’t started yet
   (e) what do you mean by "R"?

---

7. In terms of the R for the homework…
   (a) I was able to do the whole thing.
   (b) I understood the code part, but I couldn’t get the Markdown file to compile.
   (c) I didn’t understand the code at all.
   (d) I couldn’t get R or R Studio installed.
   (e) I haven't tried to work on the homework yet.

---

8.  A beta distribution[^8]
    (a) has support on [0,1]
    (b) has parameters $\alpha$ and $\beta$ which represent, respectively, the mean and variance
    (c) is discrete
    (d) has equal mean and variance
    (e) has equal mean and standard deviation
  
[^8]: (a) has support on [0,1]

---

9.  What types of distributions are the following?[^9]
    (a) prior = marginal    & posterior = joint
    (b) prior = joint       & posterior = conditional
    (c) prior = conditional & posterior = joint
    (d) prior = marginal    & posterior = conditional
    (e) prior = joint       & posterior = marginal 

[^9]: (d) prior = marginal & posterior = conditional

---

10. Which of these are incorrect conclusions?[^10]
    (a) $\theta | \underline{X} \sim$ Beta (4,12)
    (b) $\xi(\theta | \underline{X}) \sim$  Beta (4,12)
    (c) $\xi(\theta | \underline{X}) \propto$ Beta (4,12)
    (d) $\xi(\theta | \underline{X}) \propto \theta^{4-1} (1-\theta)^{12-1}$
    (e) $\xi(\theta | \underline{X}) = \frac{1}{B(4,12)} \theta^{4-1}(1-\theta)^{12-1}$ 
   
[^10]: Both (b) $\xi(\theta | \underline{X}) \sim$  Beta (4,12) and (c) $\xi(\theta | \underline{X}) \propto$ Beta (4,12) are incorrect.  (b) because the value to the left of the $\sim$ must be a random variable.  (c) because the value to the right of the $\propto$ must be a function.

---

11. What is the integrating constant for the pdf, $h(w)$?[^11]
    (a) $\frac{\Gamma(w+k)}{\Gamma(w)\Gamma(k)}$
    (b) 1/[$w^k \Gamma(k)$]
    (c) 1 / $\sqrt{2\pi k^2}$
    (d) 1/[$\Gamma(k/2)$]
    (e) 1/[$2^{k/2} \Gamma(k/2)$]

$$h(w) \propto w^{k/2-1}e^{-w/2} \ \ \ \ \ \ \ \ \ w>0$$

[^11]: (e) 1/[$2^{k/2} \Gamma(k/2)$]

---

12. Suppose the data come from an exponential distribution with a parameter whose prior is given by a gamma distribution.  The posterior is known to be conjugate, so its distribution must be in what family?[^12]  
    (a) exponential
    (b) gamma
    (c) normal
    (d) beta
    (e) Poisson

[^12]: (b) gamma

---

13. A prior is improper if[^13]
    (a) it conveys no real information.
    (b) it isn’t conjugate.
    (c) it doesn’t integrate to one.
    (d) it swears a lot.
    (e) it isn’t on your distribution sheet.

[^13]: (c) it doesn’t integrate to one.

---

14. Given a prior: $\theta \sim N(\mu_0, \nu_0^2)$  
And a data likelihood: $X | \theta \sim N(\theta, \sigma^2)$  
You collect n data values, what is your best guess of $\theta?$[^14]  
     (a) $\overline{X}$
     (b) $\mu_0$ 
     (c) $\mu_1 = \frac{\sigma^2 \mu_0 + n \nu_0^2 \overline{X}}{\sigma^2 + n \nu_0^2}$  
     (d) median of $N(\mu_1, \nu_1^2 = \frac{\sigma^2 \nu_0^2}{\sigma^2 + n \nu_0^2})$  
     (e) 47

[^14]: $\mu_1 = \frac{\sigma^2 \mu_0 + n \nu_0 \overline{X}}{\sigma^2 + n \nu_0^2}$ 
---

<!--Put in a question with x-bar and the prior mean, what is the posterior mean going to be?  Weighted average… then do it again with a huge n.

---
-->

15.  The Bayes estimator is sensitive to[^15]
     (a) the posterior mean
     (b) the prior mean
     (c) the sample size
     (d) the data values
     (e) some of the above

[^15]: (e) some of the above (the Bayes estimator **is** the posterior mean, it is sensitive to the rest of it.)

---

16. The range (output) of the Bayesian MSE includes:[^16]
     (a) theta
     (b) the data

[^16]: (b) the data

---

17. The range (output) of the frequentist MSE includes:[^17]
     (a) theta
     (b) the data

[^17]: (a) theta

---

18. To find the maximum likelihood estimator, we take the derivative of the likelihood[^18] 
    (a) with respect to $X$
    (b) with respect to $\underline{X}$
    (c) with respect to $\theta$
    (d) with respect to $f$
    (e) with respect to $\ln(f)$

[^18]: with respect to $\theta$

---

19. Consider an MLE, $\hat{\theta},$ and the related log likelihood function $L = \ln(f).$  $\delta(X)$ is another estimate of $\theta$.  Which statement is necessarily false:[^19]
    (a) L($\delta(X)$) < L($\theta$)
    (b) L($\hat{\theta}$) < L($\theta$)
    (c) L($\theta$) < L($\delta(X)$) 
    (d) L($\delta(X)$) < L($\hat{\theta}$)
    (e) L($\theta$) < L($\hat{\theta}$)

[^19]: L($\hat{\theta}$) < L($\theta$)

---

20. The MLE is popular because it[^20]
    (a) maximizes $R^2$
    (b) minimizes the sum of squared errors
    (c) has desirable sampling distribution properties
    (d) maximizes both the likelihood and the log likelihood
    (e) always exists

[^20]: (c) has desirable sampling distribution properties and (d) maximizes both the likelihood and the log likelihood (although (c) is really the reason it is popular)

---

21. MOM is popular because it:[^21]
    (a) has desirable sampling properties
    (b) is often straightforward to compute
    (c) always produces values inside the parameter space (e.g., in [0,1] for prob)
    (d) always exists
  
[^21]: (b) is often straightforward to compute (it does not always exist, look at Cauchy.  it does not always produce estimates inside the parameter space.)

---

22. The Central Limit Theorem (CLT) says:[^22]
    (a) The sample average (statistic) converges to the true average (parameter)
    (b) The sample average (statistic) converges to some point
    (c) The distribution of the sample average (statistic) converges to a normal distribution
    (d) The distribution of the sample average (statistic) converges to some distribution
    (e) I have no idea what the CLT says

[^22]: (c) The distribution of the sample average (statistic) converges to a normal distribution

---

23. A sampling distribution is[^23]
    (a) the true distribution of the data
    (b) the estimated distribution of the data
    (c) the distribution of the population
    (d) the distribution of the statistic in repeated samples
    (e) the distribution of the statistic from your one sample of data
   
[^23]: (d) the distribution of the statistic in repeated samples

---

24. The distribution of a random variable can be uniquely determined by[^24]  
    (a) the cdf: F(x)
    (b) the pdf (pmf): f(x)
    (c) the moment generating function (mgf), if it exists: $\Psi(t) = E[e^{tX}]$
    (d) the mean and variance of the distribution
    (e) more than one of the above (which ones??)
   
[^24]: (e) the cdf, the pdf/pmf, and the mgf  

---

25. A moment generating function[^25]
    (a) gives the probability of the RV at any value of X
    (b) gives all theoretical moments of the distribution
    (c) gives all sample moments of the data
    (d) gives the cumulative probability of the RV at any value of X
   
[^25]: (b) gives all theoretical moments of the distribution

---

26. The sampling distribution is important because[^26]
    (a) it describes the behavior (distribution) of the statistic
    (b) it describes the behavior (distribution) of the data
    (c) it gives us the ability to measure the likelihood of the statistic or more extreme under     particular settings (i.e. null)
    (d) it gives us the ability to make inferences about the population parameter
    (e) more than one of the above (which ones??)

[^26]: (e): (a), (c), (d)

---

27. The following result: $\frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}$
allows us to isolate and conduct inference on what parameter?[^27]
    (a) $\overline{X}$
    (b) $s$
    (c) $\mu$
    (d) $\sigma^2$
    (e) $\chi$

[^27]: (d) $\sigma^2$  (the first two are statistics, not parameters, we can't isolate $\mu$ because it isn't involved, and $\chi$ also isn't a parameter)

---

28. The following result: $\frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$
allows us to isolate and conduct inference on what parameter?[^28]
    (a) $\overline{X}$
    (b) $s$
    (c) $\mu$
    (d) $\sigma^2$
    (e) $\chi$

[^28]: (c) $\mu$ (the first two are statistics, not parameters, we can't isolate $\sigma^2$ because it isn't involved, and $\chi$ also isn't a parameter)

---

29. What would you expect the standard deviation of the t statistic to be?[^29]
    (a) a little bit less than 1
    (b) 1
    (c) a little bit more than 1
    (d) unable to tell because it depends on the sample size and the variability of the data

[^29]: (c) a little bit more than 1 (dividing by $s$ instead of $\sigma$ adds variability to the distribution)

---

30. You have a sample of size n = 50.  You sample with replacement 1000 times to get 1000 bootstrap samples. What is the sample size of each bootstrap sample?[^30]
    (a) 50
    (b) 1000

[^30]: (a) 50 observations in each bootstrap sample

---

31. You have a sample of size n = 50.  You sample with replacement 1000 times to get 1000 bootstrap samples. How many bootstrap statistics will you have?[^31]
    (a) 50
    (b) 1000
 
[^31]: (b) 1000

---

32. The bootstrap distribution of $\hat{\theta}$  is centered around the[^32]
    (a) population parameter
    (b) sample statistic
    (c) bootstrap statistic
    (d) bootstrap parameter

[^32]: (b) the sample statistic

---

33. The bootstrap theory relies on[^33]
    (a) Resampling with replacement from the original sample.
    (b) Resampling from the original sample, leaving one observation out each time (e.g., cross validation)
    (c) Estimating the population using the sample.
    (d) Permuting the data values within the sample.

[^33]: (a) Resampling with replacement from the original sample.  Although I suppose (c) is also true.

---

34. Bias of a statistic refers to[^34]
    (a) The difference between a statistic and the actual parameter
    (b) Whether or not questions were worded fairly.
    (c) The difference between a sampling distribution mean and the actual parameter.

[^34]: (c) The difference between a sampling distribution mean and the actual parameter.

---

35. The mean of a sample is 22.5. The mean of 1000 bootstrapped samples is 22.491. The bias of the bootstrap mean is[^35]
    (a) -0.009
    (b) -0.0045
    (c) -0.09
    (d)  0.009
    (e)  0.09

[^35]: (a) -0.009 Bias is what the statistic is (on average) minus the true value. Recall, we are using the data as a proxy for the population, so the "truth" is the data. So in the bootstrap setting, the average is over the bootstrapped values and the true value is the sample mean.  





